{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec9d988-132b-45a7-819b-beb9f3e5d564",
   "metadata": {},
   "source": [
    "# PEFT Meta-Llama3.1-8B on Dolphin Dataset\n",
    "\n",
    "This example shows how to do parameter efficient fine tuning (PEFT) of [Meta-Llama3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) model on [dolphin](https://huggingface.co/datasets/cognitivecomputations/dolphin) dataset using [Nemo](https://github.com/NVIDIA/NeMo) [Megatron-LM](https://github.com/NVIDIA/Megatron-LM). In this notebook, we use a [Kubeflow Pipeline (v2)](https://www.kubeflow.org/docs/components/pipelines/v2/introduction/) to run the end-to-end workflow for PEFT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d36e0b",
   "metadata": {},
   "source": [
    "## Launch a Kubeflow Notebook server\n",
    "\n",
    "We need to run this notebook in a [Kubeflow Notebooks](https://www.kubeflow.org/docs/components/notebooks/overview/) JupyterLab notebook server. [Access Kubeflow Central Dashboard](https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow/tree/master/README.md#access-kubeflow-central-dashboard-optional), and follow the [Quickstart Guide](https://www.kubeflow.org/docs/components/notebooks/quickstart-guide/) to create an instance of the default JupyterLab notebook server. Connect to the launched notebook server from within the Kubeflow Central Dashboard. Clone this git repository under the home directory on the notebook server, and open this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba195698-349f-416d-8f3f-57b1d5315c4b",
   "metadata": {},
   "source": [
    "## Persistent volumes\n",
    "\n",
    "Amazon EFS and FSx for Lustre persistent volumes are mounted at `~/pv-efs`and `~/pv-fsx`, respectively, within the notebook server, and at `/efs` and `/fsx` within the pre-training job runtime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8021224-c2ef-4b40-a79f-c6e52f4103f4",
   "metadata": {},
   "source": [
    "## Implicitly defined environment variables\n",
    "\n",
    "Following variables are implicitly defined by the [pytorch-distributed](../../../charts/machine-learning/training/pytorchjob-distributed/Chart.yaml) Helm chart for use with [Torch distributed run](https://github.com/pytorch/pytorch/blob/main/torch/distributed/run.py):\n",
    "\n",
    "1. `PET_NNODES` : Maps to `nnodes`\n",
    "2. `PET_NPROC_PER_NODE` : Maps to `nproc_per_node` \n",
    "3. `PET_NODE_RANK` : Maps to `node_rank` \n",
    "4. `PET_MASTER_ADDR`: Maps to `master_addr` \n",
    "5. `PET_MASTER_PORT`: Maps to `master_port`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ba800-5b04-4a0e-9e5c-820aa791e4d5",
   "metadata": {},
   "source": [
    "## Create Kubeflow Pipelines Client\n",
    "\n",
    "We start by creating a client for Kubeflow Pipelines. Since we are running this notebook in a JupyterLab notebook server inside the Kubeflow platform, we can discover the Kubeflow Pipelines endpoint automatically, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b40a94-c475-4748-a1d5-916f4c260bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "client = kfp.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1b9eb-f349-4550-a7b7-83363de03ea7",
   "metadata": {},
   "source": [
    "Next, we get our Kubernetes namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a63b5e-d18a-4992-a58c-8a1d13cbe74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ns = client.get_user_namespace()\n",
    "print(f\"user namespace: {ns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9f103-e597-4ff2-8aac-7e14616b27ea",
   "metadata": {},
   "source": [
    "##  PEFT Workflow\n",
    "\n",
    "Below we define the steps in the PEFT workflow. Each step in the workflow is defined as a Helm Chart config. The sequential list of Helm Chart configs  defines the complete workflow.\n",
    "\n",
    "In each Helm chart config, the Helm chart `release_name` must be unique among the Helm charts installed within the user namespace. The `repo_url` below specifies the Git repository URL for the Helm chart, and the `path` specifies the relative path within the Git repository.The `values` field specifies the Helm Chart Values used by the Helm Chart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65200a1",
   "metadata": {},
   "source": [
    "### Step 0 Specify Docker Image\n",
    "\n",
    "This notebook uses a custom Docker container image for [Nemo](https://github.com/NVIDIA/NeMo.git) [Megatron-LM](https://github.com/NVIDIA/Megatron-LM.git). See README file in this folder for more details on how to build and push the Docker container image. Be sure to set the `image` field below to the Amazon ECR URI for your Docker image. The image must be built on a build machine, not on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ''\n",
    "assert image, \"Docker image is required\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f04a64",
   "metadata": {},
   "source": [
    "### Step 1: Download Hugging Face Pre-trained Model\n",
    "\n",
    "Below, we define the Helm chart config for downloading Hugging Face pre-trained model. Specify Hugging Face access token in `hf_token` to access the gated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc8e7d-05df-4f9d-9edf-ffb8b0d58012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "release_name = \"nemo-llama31-8b-peft-dolphin\"\n",
    "\n",
    "hf_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "hf_token = ''\n",
    "assert hf_token, \"Hugging Face Token is required to access the gated model\"\n",
    "\n",
    "hf_download_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/model-prep/hf-snapshot\",\n",
    "    \"timeout\": \"5m0s\"\n",
    "}\n",
    "\n",
    "hf_download_config[\"values\"] = {\n",
    "    \"env\": [\n",
    "        {\"name\":\"HF_MODEL_ID\",\"value\":hf_model_id},\n",
    "        {\"name\":\"HF_TOKEN\",\"value\":hf_token}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e9bed",
   "metadata": {},
   "source": [
    "### Step 2: Convert HuggingFace Checkpoint to Nemo Checkpoint\n",
    "\n",
    "Below we define the Helm Chart config for converting Hugging Face pre-trained model checkpoint to Nemo checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf_to_nemo_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/data-prep/data-process\",\n",
    "    \"timeout\": \"30m0s\"\n",
    "}\n",
    "\n",
    "with open(\"hf_to_nemo.yaml\") as file:\n",
    "    hf_to_nemo_config[\"values\"] = yaml.safe_load(file)\n",
    "    hf_to_nemo_config[\"values\"][\"image\"] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6e104",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess Dolphin Dataset\n",
    "\n",
    "Below we define the Helm Chart config for preprocessing Hugging Face Dolphin dataset into the format required by Nemo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5eae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/data-prep/data-process\",\n",
    "    \"timeout\": \"30m0s\"\n",
    "}\n",
    "\n",
    "with open(\"preprocess.yaml\") as file:\n",
    "    preprocess_config[\"values\"] = yaml.safe_load(file)\n",
    "    preprocess_config[\"values\"][\"image\"] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab72fc",
   "metadata": {},
   "source": [
    "### Step 4: PEFT Fine-tuning\n",
    "\n",
    "Below we define the Helm Chart config for PEFT fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/training/pytorchjob-distributed\",\n",
    "    \"timeout\": \"30m0s\"\n",
    "}\n",
    "\n",
    "with open(\"peft.yaml\") as file:\n",
    "    peft_config[\"values\"] = yaml.safe_load(file)\n",
    "    peft_config[\"values\"][\"image\"] = image\n",
    "    peft_config[\"values\"][\"hf_token\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1b244",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate\n",
    "\n",
    "Below we define the Helm Chart Config for evaluating Peft fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d92dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_eval_config =  {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/training/pytorchjob-distributed\",\n",
    "    \"timeout\": \"30m0s\"\n",
    "}\n",
    "\n",
    "with open(\"peft_eval.yaml\") as file:\n",
    "    peft_eval_config[\"values\"] = yaml.safe_load(file)\n",
    "    peft_eval_config[\"values\"][\"image\"] = image\n",
    "    peft_eval_config[\"values\"][\"hf_token\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ac6c8",
   "metadata": {},
   "source": [
    "### Step 6: Merge PEFT Model to Base Model\n",
    "\n",
    "Below we define the Helm Chart config for merging the PEFT model weights to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_peft_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/data-prep/data-process\",\n",
    "    \"timeout\": \"30m0s\"\n",
    "}\n",
    "\n",
    "with open(\"merge_peft.yaml\") as file:\n",
    "    merge_peft_config[\"values\"] = yaml.safe_load(file)\n",
    "    merge_peft_config[\"values\"][\"image\"] = image\n",
    "    merge_peft_config[\"values\"][\"hf_token\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd70623",
   "metadata": {},
   "source": [
    "### Step 7: Convert Nemo Checkpoint to Hugging Face Checkpoint\n",
    "\n",
    "Finally, we define the Helm Chart config for converting Nemo checkpoint to Hugging Face checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_to_hf_config = {\n",
    "    \"release_name\": release_name,\n",
    "    \"namespace\": ns,\n",
    "    \"repo_url\": \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\",\n",
    "    \"path\": \"charts/machine-learning/data-prep/data-process\",\n",
    "    \"timeout\": \"1800\"\n",
    "}\n",
    "\n",
    "with open(\"nemo_to_hf.yaml\") as file:\n",
    "    nemo_to_hf_config[\"values\"] = yaml.safe_load(file)\n",
    "    nemo_to_hf_config[\"values\"][\"hf_token\"] = hf_token\n",
    "    nemo_to_hf_config[\"values\"][\"image\"] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9803a-75fd-4930-a990-619f8be89588",
   "metadata": {},
   "source": [
    "## Create a New Kubeflow Experiment \n",
    "\n",
    "Next, we create a new [Kubeflow Experiment](https://www.kubeflow.org/docs/components/pipelines/v1/concepts/experiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198caa2-35fe-4a6f-b103-d847e6ab69a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = \"peft-llama31_8b-dolphin\"\n",
    "exp_desc=\"PEFT Llama 3.1 8B on dolphin dataset\"\n",
    "exp = client.create_experiment(name=exp_name, description=exp_desc, namespace=ns)\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cf3d9-4c27-4c72-9c36-9d493fff7cb2",
   "metadata": {},
   "source": [
    "## Run the Pipeline in the Experiment\n",
    "\n",
    "To run this pipeline, we must input `arguments` with `chart_configs` list, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad2656-82c5-4814-98f7-973eb6118138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "run_name=f\"{exp_name}-run-{ts}\"\n",
    "\n",
    "pipeline_package = \"../../../../kfp/pipelines/packages/helm_charts_pipeline.yaml\"\n",
    "\n",
    "pipeline_run=client.create_run_from_pipeline_package(\n",
    "    pipeline_file=pipeline_package, \n",
    "    arguments={ \n",
    "        \"chart_configs\": [\n",
    "            hf_download_config,\n",
    "            hf_to_nemo_config,\n",
    "            preprocess_config,\n",
    "            peft_config,\n",
    "            peft_eval_config,\n",
    "            merge_peft_config,\n",
    "            nemo_to_hf_config\n",
    "        ]\n",
    "    },\n",
    "    run_name=run_name,\n",
    "    experiment_name=exp_name, \n",
    "    namespace=ns, \n",
    "    enable_caching=False, \n",
    "    service_account='default'\n",
    ")\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f0878-f65b-4253-abe1-e968690a3154",
   "metadata": {},
   "source": [
    "## What happens during the Pipeline Run\n",
    "\n",
    "You can check the Kubeflow Pipeline Run logs using the link output above. \n",
    "\n",
    "During the Pipeline Run, the Helm charts in the `chart_configs` list are installed sequentially. Each installed Helm chart is monitored to a successful completion, or failure. If any chart in the list fails, the Pipeline Run ends in a failure, otherwise, when all the charts in the list complete successfully, the Pipeline Run concludes successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
