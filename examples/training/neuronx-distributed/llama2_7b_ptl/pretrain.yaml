image: 'public.ecr.aws/neuron/pytorch-training-neuronx:2.6.0-neuronx-py310-sdk2.23.0-ubuntu22.04'
backoff_limit: 2000
ebs:
  storage: 200Gi
  mount_path: /tmp
resources:
  requests:
    "aws.amazon.com/neuron": 16 
    "vpc.amazonaws.com/efa": 8
  limits:
    "aws.amazon.com/neuron": 16 
    "vpc.amazonaws.com/efa": 8
  nnodes: 4
  nproc_per_node: 32 
  node_type: 'trn1.32xlarge' 
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
git:
  repo_url: "https://github.com/aws-neuron/neuronx-distributed.git"
  branch: main
  commit: a070deb86991affd589c48441bf819e6d4bb159b
pre_script: 
  - pip3 install --upgrade pip
  - SCRIPT_DIR=$GIT_CLONE_DIR/examples/training/llama/lightning
  - cp -r $GIT_CLONE_DIR/examples/training/llama/tp_zero1_llama_hf_pretrain/7B_config_llama2 $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/*.* $SCRIPT_DIR
  - cd $SCRIPT_DIR
  - pip3 install -r requirements.txt protobuf==3.20.0
  - pip3 install -r requirements_ptl.txt
  - LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - mkdir -p $LOGS_DIR 
  - OUTPUT_LOG=$LOGS_DIR/pretrain.log
  - CACHE_DIR=$CACHE_ROOT/$PET_NODE_RANK
  - mkdir -p $CACHE_DIR
  - mkdir -p $CKPT_ROOT
  - DATA_PATH="$DATA_ROOT/examples_datasets/wikicorpus_llama2_tokenized_4k"
  - DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - TP_DEGREE=8
  - USE_MIX_PRECISION=1
  - USE_ZERO_1=1
  - GBS=1024
  - MBS=1
  - TOTAL_STEPS=10000
  - WARMUP_STEPS=100
  - LR=3.0e-4
  - SEQ_LEN=4096
  - DP=$(($PET_NPROC_PER_NODE * $PET_NNODES / $TP_DEGREE))
  - ACC_STEPS=$(($GBS / $MBS / $DP)) 
  - STEPS_THIS_RUN=-1
  - LLAMA2_ARGS="--model_path $SCRIPT_DIR/7B_config_llama2
    --data_dir $DATA_PATH 
    --tensor_parallel_size $TP_DEGREE
    --train_batch_size $MBS
    --steps_this_run $STEPS_THIS_RUN
    --max_steps $TOTAL_STEPS
    --warmup_steps $WARMUP_STEPS
    --lr $LR
    --grad_accum_usteps $ACC_STEPS
    --seq_len $SEQ_LEN
    --use_sequence_parallel 1
    --use_selective_checkpoint 1
    --use_fp32_optimizer 1
    --use_zero1_optimizer 1
    --scheduler_type linear
    --save_checkpoint
    --checkpoint_freq=100
    --num_kept_checkpoint=1
    --save_load_xser"
  - echo DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS
  - export TPU_NUM_DEVICES=$PET_NPROC_PER_NODE
  - export TPU_CHIPS_PER_HOST_BOUNDS=$PET_NPROC_PER_NODE
  - TMP_CACHE_DIR=/tmp/cache
  - cp -r $CACHE_DIR $TMP_CACHE_DIR
  - export NEURON_CC_FLAGS="--model-type transformer --distribution-strategy=llm-training --retry_failed_compilation --cache_dir=$TMP_CACHE_DIR"
  - NEURON_COMPILE_CACHE_URL=$TMP_CACHE_DIR neuron_parallel_compile --command clear-locks
train:
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: DATA_ROOT
      value: /fsx/home/{{ .Release.Name }}
    - name: CACHE_ROOT
      value: /efs/home/{{ .Release.Name }}/cache
    - name: CKPT_ROOT
      value: "/efs/home/{{ .Release.Name }}/checkpoints"
    - name: CCOM_SOCKET_IFNAME
      value: "eth0"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name:  NEURON_FUSE_SOFTMAX
      value: "1"
    - name: NEURON_RT_STOCHASTIC_ROUNDING_EN
      value: "1"
    - name: NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS
      value: "3"
    - name: MALLOC_ARENA_MAX
      value: "64"
  command:
    -  torchrun 
  args: 
    - $DISTRIBUTED_ARGS  
    - run_llama_nxd_ptl.py
    - $LLAMA2_ARGS
    - '--checkpoint_dir $CKPT_ROOT'
    - '2>&1 | tee $OUTPUT_LOG'
