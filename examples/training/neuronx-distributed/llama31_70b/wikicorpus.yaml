image: 'public.ecr.aws/neuron/pytorch-training-neuronx:2.6.0-neuronx-py310-sdk2.23.0-ubuntu22.04'
backoff_limit: 2000
ebs:
  storage: 200Gi
  mount_path: /tmp
resources:
  requests:
    "aws.amazon.com/neuron": 1 
  limits:
    "aws.amazon.com/neuron": 1 
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
git:
  repo_url: "https://github.com/aws-neuron/neuronx-distributed.git"
  commit: e83494557cb4c5b7e185ccf6c9240bfed9a1993d
  branch: main
pre_script: 
  - pip3 install --upgrade pip
  - DATA_PATH="$DATA_ROOT/examples_datasets/wikicorpus_llama3_tokenized_8k"
  - 'if [ -d $DATA_PATH ] && [ ! -z "$(ls -A $DATA_PATH)" ]; then echo "$DATA_PATH exists and is not empty" && exit 0; fi'
  - mkdir -p $DATA_ROOT
  - SCRIPT_DIR=$GIT_CLONE_DIR/examples/training/llama/tp_pp_llama_hf_pretrain
  - cp $GIT_CLONE_DIR/examples/training/llama/requirements.txt $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/get_dataset.py $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/modeling_llama_nxd.py $SCRIPT_DIR
  - cp $SCRIPT_DIR/70B_config_llama3.1/config.json $SCRIPT_DIR
  - cp $MODEL_PATH/tokenizer_config.json $SCRIPT_DIR
  - cp $MODEL_PATH/special_tokens_map.json $SCRIPT_DIR
  - cp $MODEL_PATH/tokenizer.json $SCRIPT_DIR
  - cd $SCRIPT_DIR
  - pip3 install huggingface-hub==0.27.1 
  - mkdir -p $LOG_ROOT 
process:
  env:
    - name: HOME
      value: /tmp
    - name: XDG_CACHE_HOME
      value: /tmp
    - name: DATA_ROOT
      value: /fsx/home/{{ .Release.Name }}
    - name: MODEL_PATH
      value: /fsx/pretrained-models/meta-llama/Llama-3.1-70B
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
  command:
    -  HOME=$DATA_ROOT python3 
  args:
    - get_dataset.py 
    - '--llama-version 3'
    - '2>&1 | tee $LOG_ROOT/dataset.log'
