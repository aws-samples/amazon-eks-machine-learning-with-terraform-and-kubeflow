image: 'nvcr.io/nvidia/nemo:25.09'
framework: 'nemo2'
ebs:
  storage: 200Gi
  mount_path: /tmp
resources:
  requests:
    "nvidia.com/gpu": 8
  limits:
    "nvidia.com/gpu": 8 
  nnodes: 1
  nproc_per_node: 8
  node_type: 'g6e.48xlarge'  
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
pre_script: 
  - pip3 install --upgrade pip
  - pip3 install bert-score==0.3.13 evaluate==0.4.6
  - mkdir -p $HOME/logs/$PET_NODE_RANK
  - PROCESS_LOG=$HOME/logs/$PET_NODE_RANK/test_checkpoint.log
  - cd $SCRIPTS_DIR
train:
  env:
    - name: HOME
      value: "/efs/home/{{ .Release.Name }}"
    - name: MODEL_PATH
      value: "/fsx/pretrained-models/Qwen/Qwen3-14B"
    - name: NCCL_SOCKET_IFNAME 
      value: "^lo,docker0"
    - name: NCCL_DEBUG
      value: "WARN"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name: RDMAV_FORK_SAFE
      value: "1"
    - name: TRITON_CACHE_DIR
      value: /tmp
    - name: XDG_CACHE_HOME
      value: /tmp
    - name: SCRIPTS_DIR
      value: /etc/framework-scripts
  command:
    - torchrun
  args:
    - --nproc_per_node=$PET_NPROC_PER_NODE
    - --nnodes=$PET_NNODES
    - --node_rank=$PET_NODE_RANK
    - --master_addr=$PET_MASTER_ADDR
    - --master_port=$PET_MASTER_PORT
    - test_checkpoint.py
    - --gpus_per_node=$PET_NPROC_PER_NODE
    - --num_nodes=$PET_NNODES
    - '2>&1 | tee $PROCESS_LOG' 
