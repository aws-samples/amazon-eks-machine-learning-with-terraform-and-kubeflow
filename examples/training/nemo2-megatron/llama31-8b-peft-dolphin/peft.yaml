image:
resources:
  requests:
    "nvidia.com/gpu": 8
  limits:
    "nvidia.com/gpu": 8
  nnodes: 1 
  nproc_per_node: 8 
  node_type: 'p4d.24xlarge' 
ebs:
  storage: 400Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
inline_script:
- |+
  
pre_script: 
  - export DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - echo "DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS"
  - export LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - mkdir -p $LOGS_DIR
  - export OUTPUT_LOG=$LOGS_DIR/peft.log
  - MODEL="$MODEL_PATH/ckpt.nemo"
  - DATASET="dolphin"
  - TRAIN_DS="[$DATA_ROOT/${DATASET}_train.jsonl]"
  - VALID_DS="[$DATA_ROOT/${DATASET}_valid.jsonl]"
  - TEST_DS="[$DATA_ROOT/${DATASET}_test.jsonl]"
  - TEST_NAMES="[${DATASET}]"
  - SCHEME="lora"
  - CONCAT_SAMPLING_PROBS="[1.0]"
  - TP_SIZE=8
  - PP_SIZE=1
  - MAX_STEPS=1000
  - export HYDRA_FULL_ERROR=1
  - export PEFT_ARGS="
    trainer.devices=$PET_NPROC_PER_NODE
    trainer.num_nodes=$PET_NNODES 
    trainer.precision=bf16
    trainer.val_check_interval=100
    +trainer.limit_val_batches=100
    trainer.max_steps=$MAX_STEPS
    model.megatron_amp_O2=True
    ++model.mcore_gpt=True
    model.tensor_model_parallel_size=${TP_SIZE}
    model.pipeline_model_parallel_size=${PP_SIZE}
    model.micro_batch_size=8
    model.global_batch_size=96
    model.optim.lr=5e-4
    model.restore_from_path=${MODEL}
    model.data.train_ds.num_workers=1
    model.data.train_ds.memmap_workers=1
    model.data.train_ds.pin_memory=false
    model.data.validation_ds.num_workers=1
    model.data.validation_ds.pin_memory=false
    model.data.validation_ds.shuffle=true
    model.data.validation_ds.memmap_workers=1
    model.data.train_ds.file_names=${TRAIN_DS}
    model.data.train_ds.concat_sampling_probabilities=[1.0]
    model.data.validation_ds.file_names=${VALID_DS}
    model.peft.peft_scheme=${SCHEME}
    model.peft.lora_tuning.target_modules=[attention_qkv]
    exp_manager.name=$EXP_NAME 
    exp_manager.create_wandb_logger=False
    exp_manager.checkpoint_callback_params.mode=min
    exp_manager.explicit_log_dir=$LOG_ROOT/nemo_experiments/$EXP_NAME
    exp_manager.resume_if_exists=True
    exp_manager.resume_ignore_no_checkpoint=True
    exp_manager.create_checkpoint_callback=True
    exp_manager.checkpoint_callback_params.monitor=validation_loss
    ++exp_manager.checkpoint_callback_params.save_best_model=True
    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True
    exp_manager.early_stopping_callback_params.monitor=val_loss
    exp_manager.early_stopping_callback_params.patience=3
    exp_manager.early_stopping_callback_params.min_delta=0.001
    model.save_nemo_on_validation_end=False"
  - SCRIPT_DIR=/NeMo/examples/nlp/language_modeling/tuning
  - cd $HOME
  - 'if [ "${PET_NODE_RANK}" -eq "0" ]; then rm $DATA_ROOT/${DATASET}*.idx.* ; fi'
  - 'if [ -d $LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints ]; then rm -rf $LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints; fi'
train:
  env:
    - name: HOME
      value: /tmp
    - name: TMPDIR
      value: /tmp
    - name: TMP
      value: /tmp
    - name: MODEL_PATH
      value: /fsx/pretrained-models/meta-llama/Llama-3.1-8B
    - name: LOG_ROOT
      value: "/efs/home/{{ .Release.Name }}/logs"
    - name: DATA_ROOT
      value: "/fsx/home/{{ .Release.Name }}/data"
    - name: CUDA_DEVICE_MAX_CONNECTIONS
      value: "1"
    - name: NCCL_SOCKET_IFNAME 
      value: "^lo,docker0"
    - name: NCCL_DEBUG 
      value: "WARN"
    - name: EXP_NAME
      value: "peft_dolphin"
    - name: HF_TOKEN
      value: "{{ .Values.hf_token }}"
  command:
    -  "torchrun"
  args: 
    - $DISTRIBUTED_ARGS
    - $SCRIPT_DIR/megatron_gpt_finetuning.py
    - $PEFT_ARGS
    - '2>&1 | tee $OUTPUT_LOG' 
