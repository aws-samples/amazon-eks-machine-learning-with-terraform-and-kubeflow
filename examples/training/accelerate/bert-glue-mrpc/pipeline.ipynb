{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec9d988-132b-45a7-819b-beb9f3e5d564",
   "metadata": {},
   "source": [
    "# Pre-train BERT using Hugging Face Accelerate library\n",
    "\n",
    "This example illustrates how to [pre-train BERT on Glue MRPC dataset](https://github.com/huggingface/accelerate/blob/main/examples/complete_nlp_example.py) with [Hugging Face Accelerate](https://github.com/huggingface/accelerate) library, using a [Kubeflow Pipeline (v2)](https://www.kubeflow.org/docs/components/pipelines/v2/introduction/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d36e0b",
   "metadata": {},
   "source": [
    "## Launch a JupyterLab notebook server\n",
    "\n",
    "We need to run this notebook in a [Kubeflow Notebooks](https://www.kubeflow.org/docs/components/notebooks/overview/) JupyterLab notebook server. [Access Kubeflow Central Dashboard](https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow/tree/master/README.md#access-kubeflow-central-dashboard-optional), and follow the [Quickstart Guide](https://www.kubeflow.org/docs/components/notebooks/quickstart-guide/) to create an instance of the default JupyterLab notebook server. Connect to the launched notebook server from within the Kubeflow Central Dashboard. Clone this git repository under the home directory on the notebook server, and open this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba195698-349f-416d-8f3f-57b1d5315c4b",
   "metadata": {},
   "source": [
    "## Persistent volumes\n",
    "\n",
    "Amazon EFS and FSx for Lustre persistent volumes are mounted at `~/pv-efs`and `~/pv-fsx`, respectively, within the notebook server, and at `/efs` and `/fsx` within the pre-training job runtime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8021224-c2ef-4b40-a79f-c6e52f4103f4",
   "metadata": {},
   "source": [
    "## Implicitly defined environment variables\n",
    "\n",
    "Following variables are implicitly defined by the [PyTorch Elastic](https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow/tree/master/charts/machine-learning/training/pytorchjob-elastic/) Helm chart for use with [Torchrun elastic launch](https://pytorch.org/docs/stable/elastic/run.html):\n",
    "\n",
    "1. `PET_NNODES` : Torchrun elastic launch`nnodes`\n",
    "2. `PET_NPROC_PER_NODE` : Torchrun elastic launch `nproc_per_node` \n",
    "3. `PET_RDZV_ID` : Torchrun elastic launch `rdzv_id` \n",
    "4. `PET_RDZV_ENDPOINT`: Torchrun elastic launch `rdzv_endpoint`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ba800-5b04-4a0e-9e5c-820aa791e4d5",
   "metadata": {},
   "source": [
    "## Create Kubeflow Pipelines Client\n",
    "\n",
    "We start by creating a client for Kubeflow Pipelines. Since we are running this notebook in a JupyterLab notebook server inside the Kubeflow platform, we can discover the Kubeflow Pipelines endpoint automatically, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b40a94-c475-4748-a1d5-916f4c260bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "client = kfp.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1b9eb-f349-4550-a7b7-83363de03ea7",
   "metadata": {},
   "source": [
    "Next, we get our Kubernetes namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a63b5e-d18a-4992-a58c-8a1d13cbe74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ns = client.get_user_namespace()\n",
    "print(f\"user namespace: {ns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9f103-e597-4ff2-8aac-7e14616b27ea",
   "metadata": {},
   "source": [
    "## Define pre-training configuration\n",
    "\n",
    "In this step, we define the configuration for the pre-training job. This configuration specifies the Git repo URL for the  Helm chart, and specifies the input values used to install the Helm chart. The input values are loaded from the [pretrain.yaml](./pretrain.yaml) file.\n",
    "\n",
    "The Helm chart `release_name` must be unique among the Helm charts installed within the user namespace. The `repo_url` below specifies the Git repository URL for the Helm chart, and the `path` specifies the relative path within the Git repository. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc8e7d-05df-4f9d-9edf-ffb8b0d58012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "# Release name must be unique within the namespace\n",
    "release_name = \"accel-bert\"\n",
    "\n",
    "pretrain_config_yaml = yaml.safe_load(f\"\"\"\n",
    "release_name: {release_name}\n",
    "namespace: {ns}\n",
    "repo_url: \"https://github.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow.git\"\n",
    "path: \"charts/machine-learning/training/pytorchjob-elastic\"\n",
    "\"\"\")\n",
    "\n",
    "with open(\"pretrain.yaml\") as file:\n",
    "    pretrain_config_yaml[\"values\"] = yaml.safe_load(file)\n",
    "    \n",
    "pretrain_config_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9803a-75fd-4930-a990-619f8be89588",
   "metadata": {},
   "source": [
    "## Create a New Kubeflow Experiment \n",
    "\n",
    "Next, we create a new [Kubeflow Experiment](https://www.kubeflow.org/docs/components/pipelines/v1/concepts/experiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198caa2-35fe-4a6f-b103-d847e6ab69a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = \"accel-bert-experiment-1\"\n",
    "exp_desc=\"Pre-train BERT on Glue MRPC dataset using Hugging Face Accelerate\"\n",
    "exp = client.create_experiment(name=exp_name, description=exp_desc, namespace=ns)\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cf3d9-4c27-4c72-9c36-9d493fff7cb2",
   "metadata": {},
   "source": [
    "## Run the Pipeline in the Experiment\n",
    "\n",
    "To run this pipeline, we must input `arguments` with `chart_configs` list, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad2656-82c5-4814-98f7-973eb6118138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "run_name=f\"{exp_name}-run-{ts}\"\n",
    "\n",
    "pipeline_package = \"../../../../kfp/pipelines/packages/helm_charts_pipeline.yaml\"\n",
    "\n",
    "pipeline_run=client.create_run_from_pipeline_package(pipeline_file=pipeline_package, \n",
    "                                 arguments={ \"chart_configs\": [pretrain_config_yaml]},\n",
    "                                 run_name=run_name,\n",
    "                                 experiment_name=exp_name, \n",
    "                                 namespace=ns, \n",
    "                                 enable_caching=False, \n",
    "                                 service_account='default')\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f0878-f65b-4253-abe1-e968690a3154",
   "metadata": {},
   "source": [
    "## What happens during the Pipeline Run\n",
    "\n",
    "You can check the Kubeflow Pipeline Run logs using the link output above. \n",
    "\n",
    "During the Pipeline Run, the Helm charts in the `chart_configs` list are installed sequentially. Each installed Helm chart is monitored to a successful completion, or failure. If any chart in the list fails, the Pipeline Run ends in a failure, otherwise, when all the charts in the list complete successfully, the Pipeline Run concludes successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d032a-9021-4565-a6c1-a04671530ccc",
   "metadata": {},
   "source": [
    "## Tail log output\n",
    "\n",
    "The pre-training job log output (when the job is running) is available on the EFS file-system, as shown below. You can expect a delay of about 5-minutes before the pre-training job logout is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a9bcb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tail -f ~/pv-efs/home/{release_name}/logs/pytorchjob-{release_name}-worker-0/pretrain.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
