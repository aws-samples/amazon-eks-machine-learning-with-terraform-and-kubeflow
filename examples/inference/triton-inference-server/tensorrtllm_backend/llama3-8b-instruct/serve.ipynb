{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Llama3 8B Instruct using Triton Inference Server with TensorRT-LLM\n",
    "\n",
    "This notebook shows how to serve [Meta Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) with [TensorRT-LLM backend](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from kubernetes import client, config\n",
    "\n",
    "# Load Kubernetes configuration\n",
    "config.load_kube_config()\n",
    "v1 = client.CoreV1Api()\n",
    "\n",
    "def find_matching_helm_pods(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find pods managed by a specific Helm release\"\"\"\n",
    "    helm_pods = v1.list_namespaced_pod(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_pods = []\n",
    "    for pod in helm_pods.items:\n",
    "        if (pod.metadata.annotations and\n",
    "            pod.metadata.annotations.get('app.kubernetes.io/managed-by') == 'Helm' and \n",
    "            pod.metadata.annotations.get('app.kubernetes.io/instance') == release_name):\n",
    "            matching_pods.append(pod)\n",
    "\n",
    "    return matching_pods\n",
    "\n",
    "def wait_for_helm_release_pods(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for all pods in a helm release to complete successfully\"\"\"\n",
    "    print(f\"Waiting for pods in release '{release_name}' to complete...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            matching_pods = find_matching_helm_pods(release_name, namespace)\n",
    "            \n",
    "            if not matching_pods:\n",
    "                print(f\"No pods found in Hem release: {release_name} waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            all_completed = True\n",
    "            for pod in matching_pods:\n",
    "                status = pod.status.phase\n",
    "                print(f\"Pod {pod.metadata.name}: {status}\")\n",
    "                \n",
    "                if status in ['Pending', 'Running']:\n",
    "                    all_completed = False\n",
    "                elif status == 'Failed':\n",
    "                    print(f\"Pod {pod.metadata.name} failed!\")\n",
    "                    return False\n",
    "            \n",
    "            if all_completed:\n",
    "                print(\"All pods completed successfully!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking pods: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for pods to complete\")\n",
    "    return False\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Access the region_name attribute to get the current region\n",
    "current_region = session.region_name\n",
    "\n",
    "cmd = ['./containers/tritonserver-trtllm/build_tools/build_and_push.sh', current_region]\n",
    "\n",
    "# Start the subprocess with streaming output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')  # end='' prevents double newlines\n",
    "    sys.stdout.flush()   # Force immediate output\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code != 0:\n",
    "    print(f\"\\nProcess exited with return code: {return_code}\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Hugging Face Llama3 8B Instruct Model Weights\n",
    "\n",
    "**Note:** Set your Hugging Face token below before running cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual Hugging Face token\n",
    "HF_TOKEN = None\n",
    "assert HF_TOKEN, \"Please set HF_TOKEN\"\n",
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-llama3-8b-instruct-trtllm',\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"meta-llama/Meta-Llama-3-8B-Instruct\"}},{{\"name\":\"HF_TOKEN\",\"value\":\"{HF_TOKEN}\"}}]',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for model download to complete\n",
    "wait_for_helm_release_pods('triton-server-llama3-8b-instruct-trtllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the model download job\n",
    "cmd = ['helm', 'uninstall', 'triton-server-llama3-8b-instruct-trtllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert HuggingFace Checkpoint to TensorRT-LLM Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-llama3-8b-instruct-trtllm',\n",
    "    'charts/machine-learning/data-prep/data-process',\n",
    "    '-f', 'examples/inference/triton-inference-server/tensorrtllm_backend/llama3-8b-instruct/hf_to_trtllm.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for checkpoint conversion to complete\n",
    "wait_for_helm_release_pods('triton-server-llama3-8b-instruct-trtllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the checkpoint conversion job\n",
    "cmd = ['helm', 'uninstall', 'triton-server-llama3-8b-instruct-trtllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build TensorRT-LLM Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-llama3-8b-instruct-trtllm',\n",
    "    'charts/machine-learning/data-prep/data-process',\n",
    "    '-f', 'examples/inference/triton-inference-server/tensorrtllm_backend/llama3-8b-instruct/trtllm_engine.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for engine build to complete\n",
    "wait_for_helm_release_pods('triton-server-llama3-8b-instruct-trtllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the engine build job\n",
    "cmd = ['helm', 'uninstall', 'triton-server-llama3-8b-instruct-trtllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Triton Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-llama3-8b-instruct-trtllm',\n",
    "    'charts/machine-learning/data-prep/data-process',\n",
    "    '-f', 'examples/inference/triton-inference-server/tensorrtllm_backend/llama3-8b-instruct/triton_model.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Triton model build to complete\n",
    "wait_for_helm_release_pods('triton-server-llama3-8b-instruct-trtllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the Triton model build job\n",
    "cmd = ['helm', 'uninstall', 'triton-server-llama3-8b-instruct-trtllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Launch Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-llama3-8b-instruct-trtllm',\n",
    "    'charts/machine-learning/serving/triton-inference-server',\n",
    "    '-f', 'examples/inference/triton-inference-server/tensorrtllm_backend/llama3-8b-instruct/triton_server.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Triton server to be ready\n",
    "def wait_for_triton_server(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for Triton server pods to be running and ready\"\"\"\n",
    "    print(f\"Waiting for Triton server '{release_name}' to be ready...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            matching_pods = find_matching_helm_pods(release_name, namespace)\n",
    "            \n",
    "            if not matching_pods:\n",
    "                print(f\"No pods found in Hem release: {release_name} waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            all_ready = True\n",
    "            for pod in matching_pods:\n",
    "                status = pod.status.phase\n",
    "                ready = all(condition.status == 'True' for condition in pod.status.conditions if condition.type == 'Ready')\n",
    "                print(f\"Pod {pod.metadata.name}: {status}, Ready: {ready}\")\n",
    "                \n",
    "                if status != 'Running' or not ready:\n",
    "                    all_ready = False\n",
    "                elif status == 'Failed':\n",
    "                    print(f\"Pod {pod.metadata.name} failed!\")\n",
    "                    return False\n",
    "            \n",
    "            if all_ready:\n",
    "                print(\"Triton Inference Server Pod is Ready!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking pods: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for Triton Inference Server to be Ready\")\n",
    "    return False\n",
    "\n",
    "wait_for_triton_server('triton-server-llama3-8b-instruct-trtllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_helm_services(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find services managed by a specific Helm release\"\"\"\n",
    "    helm_services = v1.list_namespaced_service(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_services = []\n",
    "    for service in helm_services.items:\n",
    "        if (service.metadata.annotations and\n",
    "            service.metadata.annotations.get('app.kubernetes.io/managed-by') == 'Helm' and\n",
    "            service.metadata.annotations.get('app.kubernetes.io/instance') == release_name):\n",
    "            matching_services.append(service)\n",
    "\n",
    "    return matching_services\n",
    "\n",
    "# Check service status\n",
    "services = find_matching_helm_services('triton-server-llama3-8b-instruct-trtllm')\n",
    "print(services)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Stop Service\n",
    "\n",
    "When you're done with the service, run this cell to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['helm', 'uninstall', 'triton-server-llama3-8b-instruct-trtllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
