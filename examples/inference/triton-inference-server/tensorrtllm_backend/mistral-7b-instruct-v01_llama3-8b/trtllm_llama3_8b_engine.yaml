image: 
resources:
  node_type: p4d.24xlarge
  requests:
    "nvidia.com/gpu": 8
  limits:
    "nvidia.com/gpu": 8
ebs:
  storage: 400Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
pre_script: 
  - mkdir -p $LOG_ROOT
  - TP_SIZE=8
  - PP_SIZE=1
  - MODEL_NAME=llama3_8b_instruct
  - OUTPUT_LOG=$LOG_ROOT/${MODEL_NAME}_build_trtllm_tp_${TP_SIZE}_pp_${PP_SIZE}.log
  - CKPT_PATH=$OUTPUT_ROOT/${MODEL_NAME}_ckpt_tp_${TP_SIZE}_pp_${PP_SIZE}
  - TMP_ENGINE_DIR=/tmp/${MODEL_NAME}_engine_tp_${TP_SIZE}_pp_${PP_SIZE}
post_script:
  - rm -rf $OUTPUT_ROOT/${MODEL_NAME}_engine_tp_${TP_SIZE}_pp_${PP_SIZE}
  - cp -r $TMP_ENGINE_DIR $OUTPUT_ROOT/
process:
  env:
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: OUTPUT_ROOT
      value: /efs/home/{{ .Release.Name }}/trtllm
    - name: MODEL_PATH
      value: /fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct
  command:
    - trtllm-build
  args:
    - --checkpoint_dir ${CKPT_PATH}
    - --max_num_tokens 32768
    - --gpus_per_node 8
    - --remove_input_padding enable
    - --paged_kv_cache enable
    - --context_fmha enable
    - --output_dir ${TMP_ENGINE_DIR}
    - --max_batch_size 8
    - '2>&1 | tee $OUTPUT_LOG'
