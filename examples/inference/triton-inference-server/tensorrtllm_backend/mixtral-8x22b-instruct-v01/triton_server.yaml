image: 
  name: 
  pull_policy: Always
lws:
  size: 2
resources:
  node_type: p4d.24xlarge
  requests:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  limits:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
ebs:
  storage: 400Gi
  mount_path: /tmp
inline_script:
  - |+
    cat > /tmp/hf_to_trtllm.sh <<'EOF'
    #!/bin/bash
    set -xe
    mkdir -p $LOG_ROOT
    TP_SIZE=8
    PP_SIZE=2
    OUTPUT_LOG=$LOG_ROOT/hf_to_trtllm.log
    OUTPUT_PATH=/tmp/ckpt
    SCRIPT_DIR=TensorRT-LLM/examples/models/core/llama
    cd $SCRIPT_DIR
    pip3 install datasets==3.1.0 evaluate~=0.4.3 rouge_score~=0.1.2 sentencepiece~=0.2.0
    python3 convert_checkpoint.py --model_dir=$MODEL_PATH --output_dir=$OUTPUT_PATH --dtype=auto --tp_size=$TP_SIZE --pp_size=$PP_SIZE 2>&1 | tee $OUTPUT_LOG
    EOF
  - |+
    cat > /tmp/trtllm_engine.sh <<'EOF'
    #!/bin/bash
    set -xe
    mkdir -p $LOG_ROOT
    OUTPUT_LOG=$LOG_ROOT/build_trtllm.log
    CKPT_PATH=/tmp/ckpt
    ENGINE_DIR=/tmp/engine
    trtllm-build --checkpoint_dir ${CKPT_PATH} --max_num_tokens 16384 --gpus_per_node 8 --remove_input_padding enable --paged_kv_cache enable --context_fmha enable --output_dir ${ENGINE_DIR} --max_batch_size 8 2>&1 | tee $OUTPUT_LOG
    EOF
  - |+
    cat > /tmp/triton_model.sh <<'EOF'
    #!/bin/bash
    set -xe
    SCRIPT_DIR=TensorRT-LLM/triton_backend
    cd $SCRIPT_DIR
    mkdir -p $LOG_ROOT
    OUTPUT_LOG=$LOG_ROOT/triton_model.log
    TOKENIZER_DIR=$MODEL_PATH
    DECOUPLED_MODE=false
    MODEL_REPO=/tmp/model_repo
    MAX_BATCH_SIZE=8
    INSTANCE_COUNT=1
    MAX_QUEUE_DELAY_MS=1000000
    FILL_TEMPLATE_SCRIPT=tools/fill_template.py
    ENGINE_DIR=/tmp/engine
    mkdir -p $MODEL_REPO
    rm -rf $MODEL_REPO/*
    cp -r all_models/inflight_batcher_llm/* $MODEL_REPO/
    ENCODER_INPUT_DATA_TYPE=TYPE_BF16
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT} 2>&1 | tee $OUTPUT_LOG
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT} 2>&1 | tee -a $OUTPUT_LOG
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT} 2>&1 | tee -a $OUTPUT_LOG
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_REPO}/ensemble/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE} 2>&1 | tee -a $OUTPUT_LOG
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_REPO}/tensorrt_llm/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,encoder_input_features_data_type:${ENCODER_INPUT_DATA_TYPE},triton_backend:tensorrtllm 2>&1 | tee -a $OUTPUT_LOG
    sed -i 's/${logits_datatype}/TYPE_FP32/g' ${MODEL_REPO}/ensemble/config.pbtxt
    sed -i 's/${logits_datatype}/TYPE_FP32/g' ${MODEL_REPO}/tensorrt_llm/config.pbtxt
    sed -i 's/${logits_datatype}/TYPE_FP32/g' ${MODEL_REPO}/tensorrt_llm_bls/config.pbtxt
    EOF
pre_script: 
  - set -xe
  - bash /tmp/hf_to_trtllm.sh
  - bash /tmp/trtllm_engine.sh
  - bash /tmp/triton_model.sh
  - mkdir -p $LOG_ROOT
  - TP_SIZE=8
  - PP_SIZE=2
  - OUTPUT_LOG="$LOG_ROOT/triton_server.log"
  - MODEL_REPO=/tmp/model_repo
server:
  ports:
    - name: 'http'
      value: '8000'
    - name: 'grpc'
      value: '8001'
    - name: 'metric'
      value: '8002'
  readiness_probe:
    period_secs: 10
    failure_threshold: 3
  startup_probe:
    period_secs: 10
    failure_threshold: 360
  liveness_probe:
    period_secs: 10
    failure_threshold: 3
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_PATH
      value: /fsx/pretrained-models/mistralai/Mixtral-8x22B-Instruct-v0.1
    - name: NCCL_SOCKET_IFNAME 
      value: "^lo,docker0"
    - name: NCCL_DEBUG
      value: "WARN"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name: "RDMAV_FORK_SAFE"
      value: "1"
  command:
    - python3
  args:
    - server.py
    - --tp=${TP_SIZE}
    - --pp=${PP_SIZE}
    - --model-repo=${MODEL_REPO}
    - --model=tensorrt_llm
    - --grpc_port=8001
    - --http_port=8000
    - --metrics_port=8002
    - --log-file=$OUTPUT_LOG
    - --namespace=${NAMESPACE}
    - --group-key=${GROUP_KEY}
  autoscaling:
    minReplicas: 1
    maxReplicas: 1
    metrics:
      - type: Pods
        pods:
          metric:
            name: avg_time_queue_us
          target:
            type: AverageValue
            averageValue: 50
