{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve BAAI Bge Reranker Large using Triton Inference Server on AWS Neuron\n",
    "\n",
    "This notebook shows how to serve [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) model using [Triton Inference Server](https://github.com/triton-inference-server) on [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) with [torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Get the src directory\n",
    "src_dir = os.path.join(os.getcwd(), \"src\")\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from k8s.utils import (\n",
    "    wait_for_helm_release_pods,\n",
    "    wait_for_triton_server,\n",
    "    find_matching_helm_services\n",
    ")\n",
    "\n",
    "# Get notebook directory\n",
    "notebook_dir = os.path.join(os.getcwd(), 'examples', 'inference', \n",
    "    'triton-inference-server', 'python_backend', 'baai-bge-reranker-large-neuron')\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "\n",
    "# initialize key variables\n",
    "release_name = 'triton-server-baai-bge-reranker-large-neuron'\n",
    "namespace = 'kubeflow-user-example-com'\n",
    "hf_model_id = 'BAAI/bge-reranker-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push Docker Container\n",
    "\n",
    "Build and push Docker container image to your current AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Access the region_name attribute to get the current region\n",
    "current_region = session.region_name\n",
    "\n",
    "cmd = ['./containers/tritonserver-neuronx/build_tools/build_and_push.sh', current_region]\n",
    "\n",
    "# Start the subprocess with streaming output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')  # end='' prevents double newlines\n",
    "    sys.stdout.flush()   # Force immediate output\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code != 0:\n",
    "    print(f\"\\nProcess exited with return code: {return_code}\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Hugging Face BAAI Bge Reranker Large Model Weights\n",
    "\n",
    "Below we download the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"{hf_model_id}\"}}]',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for model download to complete\n",
    "wait_for_helm_release_pods(release_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the model download job\n",
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Launch Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/serving/triton-inference-server',\n",
    "    '-f', f'{notebook_dir}/triton_server.yaml',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_triton_server(release_name, namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "services = find_matching_helm_services(release_name, namespace)\n",
    "for service in services:\n",
    "    print(f\"Service {service.metadata.name} is available.\")\n",
    "    print(f\"Service type: {service.spec.type}\")\n",
    "    print(f\"Service ports: {service.spec.ports} \")\n",
    "    print(f\"Run  'kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}' in a separate terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Deployed Reranker Model\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `kubectl port-forward svc/YOUR_SERVICE_NAME 8000:8000 -n YOUR_NAMESPACE` in a separate terminal\n",
    "- Install required packages: `pip install requests numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for testing\n",
    "! pip install requests numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Configuration for testing\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "MODEL_NAME = \"baai-bge-reranker-large\"  # Update this based on your model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model is Ready\n",
    "\n",
    "Below we check Triton Inference server is healthy, and the reranker model is successfully deployed within the server, and is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_server_health(base_url: str = BASE_URL) -> bool:\n",
    "    \"\"\"Check if the Triton server is healthy and responsive\"\"\"\n",
    "    try:\n",
    "        health_url = f\"{base_url}/v2/health/ready\"\n",
    "        response = requests.get(health_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úì Triton server is healthy and ready\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚úó Triton server health check failed: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚úó Cannot connect to Triton server: {e}\")\n",
    "        print(\"\\nPlease ensure kubectl port-forward is running:\")\n",
    "        print(f\"kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}\")\n",
    "        return False\n",
    "\n",
    "def check_model_ready(base_url: str = BASE_URL, model_name: str = MODEL_NAME) -> List[str]:\n",
    "    \"\"\"Check model is ready\"\"\"\n",
    "    try:\n",
    "        model_url = f\"{base_url}/v2/models/{model_name}/ready\"\n",
    "        response = requests.get(model_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"Available model: {model_name}\")\n",
    "            return [model_name]\n",
    "        else:\n",
    "            print(f\"Failed to check model readiness: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Cannot check model readiness: {e}\")\n",
    "        return []\n",
    "\n",
    "# Check server health and model is ready\n",
    "server_healthy = check_server_health()\n",
    "if server_healthy:\n",
    "    available_models = check_model_ready()\n",
    "else:\n",
    "    available_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tests for Reranker Model\n",
    "\n",
    "Below we define the tests for the reranker model. The reranker takes a query and multiple documents, then returns relevance scores for ranking the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reranker(query: str, documents: List[str], model_name: str = MODEL_NAME, base_url: str = BASE_URL) -> Dict[str, Any]:\n",
    "    \"\"\"Test reranker with a query and list of documents\"\"\"\n",
    "    \n",
    "    # Triton inference endpoint\n",
    "    url = f\"{base_url}/v2/models/{model_name}/infer\"\n",
    "    \n",
    "    # Prepare the request payload for Triton reranker\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"query\",\n",
    "                \"shape\": [1, 1],  # batch_size=1, single query\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [query]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"texts\",\n",
    "                \"shape\": [1, len(documents)],  # batch_size=1, number of documents\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": documents\n",
    "            }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"name\": \"scores\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send request to Triton server\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        \n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"num_documents\": len(documents),\n",
    "            \"documents\": documents,\n",
    "            \"status_code\": response.status_code,\n",
    "            \"success\": response.status_code == 200\n",
    "        }\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract scores from response\n",
    "            if \"outputs\" in response_data and len(response_data[\"outputs\"]) > 0:\n",
    "                scores_output = response_data[\"outputs\"][0]\n",
    "                scores_data = scores_output[\"data\"]\n",
    "                scores_shape = scores_output[\"shape\"]\n",
    "                \n",
    "                # Convert to numpy array for analysis\n",
    "                scores_array = np.array(scores_data)\n",
    "                \n",
    "                result.update({\n",
    "                    \"scores_shape\": scores_shape,\n",
    "                    \"scores_datatype\": scores_output.get(\"datatype\", \"Unknown\"),\n",
    "                    \"scores\": scores_array.tolist(),\n",
    "                    \"scores_stats\": {\n",
    "                        \"min\": float(scores_array.min()),\n",
    "                        \"max\": float(scores_array.max()),\n",
    "                        \"mean\": float(scores_array.mean()),\n",
    "                        \"std\": float(scores_array.std()) if len(scores_array) > 1 else 0.0\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                # Create ranked results\n",
    "                doc_scores = list(zip(documents, scores_array))\n",
    "                ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                result[\"ranked_documents\"] = [\n",
    "                    {\n",
    "                        \"rank\": i + 1,\n",
    "                        \"document\": doc,\n",
    "                        \"score\": float(score),\n",
    "                        \"document_preview\": doc[:100] + \"...\" if len(doc) > 100 else doc\n",
    "                    }\n",
    "                    for i, (doc, score) in enumerate(ranked_docs)\n",
    "                ]\n",
    "            else:\n",
    "                result[\"error\"] = \"No outputs found in response\"\n",
    "                result[\"raw_response\"] = response_data\n",
    "        else:\n",
    "            result[\"error\"] = response.text\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        result[\"error\"] = f\"Request failed: {e}\"\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Unexpected error: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define test cases for reranker\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"documents\": [\n",
    "            \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\",\n",
    "            \"The weather today is sunny with a temperature of 75 degrees Fahrenheit.\",\n",
    "            \"Machine learning algorithms can be supervised, unsupervised, or reinforcement learning based on the type of data and learning approach.\",\n",
    "            \"Cooking pasta requires boiling water and adding salt for flavor.\",\n",
    "            \"Deep learning is a specialized branch of machine learning that uses neural networks with multiple layers to process complex patterns in data.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do pandas eat bamboo?\",\n",
    "        \"documents\": [\n",
    "            \"Giant pandas have a specialized thumb-like structure that helps them grasp bamboo stalks while eating.\",\n",
    "            \"Pandas spend up to 14 hours a day eating bamboo, consuming up to 40 pounds daily to meet their nutritional needs.\",\n",
    "            \"The stock market experienced significant volatility last week due to economic uncertainty.\",\n",
    "            \"Pandas have strong jaw muscles and flat molars that are perfectly adapted for crushing and grinding tough bamboo fibers.\",\n",
    "            \"Python programming language is popular for data science and machine learning applications.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Benefits of renewable energy\",\n",
    "        \"documents\": [\n",
    "            \"Solar and wind energy are clean, sustainable sources that reduce greenhouse gas emissions and combat climate change.\",\n",
    "            \"Fast food restaurants often use processed ingredients that may not be the healthiest option for regular consumption.\",\n",
    "            \"Renewable energy sources like hydroelectric, solar, and wind power provide energy independence and reduce reliance on fossil fuels.\",\n",
    "            \"The latest smartphone features include improved camera quality and longer battery life for enhanced user experience.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_cases)} reranker examples with model: {MODEL_NAME}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Reranker Tests\n",
    "\n",
    "Now we run the defined reranker tests to verify the model correctly ranks documents by relevance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all examples\n",
    "if server_healthy and available_models:\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        query = test_case[\"query\"]\n",
    "        documents = test_case[\"documents\"]\n",
    "        \n",
    "        print(f\"\\nTest {i}/{len(test_cases)}: {query}\")\n",
    "        print(f\"Documents to rank: {len(documents)}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = test_reranker(query, documents)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úì Success - Ranked {result['num_documents']} documents\")\n",
    "            print(f\"  Scores shape: {result['scores_shape']}\")\n",
    "            print(f\"  Score stats: min={result['scores_stats']['min']:.4f}, max={result['scores_stats']['max']:.4f}, mean={result['scores_stats']['mean']:.4f}\")\n",
    "            \n",
    "            # Show top 3 ranked documents\n",
    "            print(\"\\n  Top 3 ranked documents:\")\n",
    "            for rank_info in result['ranked_documents'][:3]:\n",
    "                print(f\"    {rank_info['rank']}. Score: {rank_info['score']:.4f}\")\n",
    "                print(f\"       {rank_info['document_preview']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"‚úó Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Summary\n",
    "    successful_tests = sum(1 for r in results if r['success'])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Summary: {successful_tests}/{len(results)} tests passed\")\n",
    "    \n",
    "    if successful_tests == len(results):\n",
    "        print(\"üéâ All tests passed! Your reranker model is working correctly.\")\n",
    "        print(\"\\nThe model successfully:\")\n",
    "        print(\"- Processed queries with multiple documents\")\n",
    "        print(\"- Generated relevance scores for ranking\")\n",
    "        print(\"- Ranked documents by relevance to the query\")\n",
    "    elif successful_tests > 0:\n",
    "        print(\"‚ö†Ô∏è  Some tests passed, but there were failures. Check the errors above.\")\n",
    "    else:\n",
    "        print(\"‚ùå All tests failed. Please check your model deployment and configuration.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot run tests - server not healthy or no models available\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure your reranker model is deployed via Helm\")\n",
    "    print(\"2. Check that kubectl port-forward is running:\")\n",
    "    print(f\"   kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}\")\n",
    "    print(\"3. Verify the service is running:\")\n",
    "    print(f\"   kubectl get pods -n {namespace}\")\n",
    "    print(\"4. Check model name matches your deployment configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Reranker Analysis\n",
    "\n",
    "Let's run some additional analysis to better understand the reranker's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis if tests were successful\n",
    "if server_healthy and available_models and results and all(r['success'] for r in results):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RERANKER ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (test_case, result) in enumerate(zip(test_cases, results), 1):\n",
    "        print(f\"\\nAnalysis {i}: {result['query']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Show score distribution\n",
    "        scores = result['scores']\n",
    "        print(f\"Score distribution: {scores}\")\n",
    "        \n",
    "        # Identify most and least relevant documents\n",
    "        ranked_docs = result['ranked_documents']\n",
    "        most_relevant = ranked_docs[0]\n",
    "        least_relevant = ranked_docs[-1]\n",
    "        \n",
    "        print(f\"\\nMost relevant (Score: {most_relevant['score']:.4f}):\")\n",
    "        print(f\"  {most_relevant['document'][:150]}...\")\n",
    "        \n",
    "        print(f\"\\nLeast relevant (Score: {least_relevant['score']:.4f}):\")\n",
    "        print(f\"  {least_relevant['document'][:150]}...\")\n",
    "        \n",
    "        # Calculate score spread\n",
    "        score_spread = most_relevant['score'] - least_relevant['score']\n",
    "        print(f\"\\nScore spread: {score_spread:.4f} (higher spread indicates better discrimination)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Reranker model analysis complete!\")\n",
    "    print(\"The model demonstrates ability to:\")\n",
    "    print(\"- Distinguish between relevant and irrelevant documents\")\n",
    "    print(\"- Provide meaningful relevance scores\")\n",
    "    print(\"- Rank documents in order of relevance to queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stop Service\n",
    "\n",
    "When you're done with the service, run this cell to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
