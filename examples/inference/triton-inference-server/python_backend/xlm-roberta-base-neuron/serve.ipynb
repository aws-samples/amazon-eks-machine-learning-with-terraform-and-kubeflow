{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve FacebookAI XLM Roberta Base using Triton Inference Server on AWS Neuron\n",
    "\n",
    "This notebook shows how to serve [FacebookAI/xlm-roberta-base](https://huggingface.co/FacebookAI/xlm-roberta-base) model using [Triton Inference Server](https://github.com/triton-inference-server) on [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) with [torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Get the src directory\n",
    "src_dir = os.path.join(os.getcwd(), \"src\")\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from k8s.utils import (\n",
    "    wait_for_helm_release_pods,\n",
    "    wait_for_triton_server,\n",
    "    find_matching_helm_services\n",
    ")\n",
    "\n",
    "# Get notebook directory\n",
    "notebook_dir = os.path.join(os.getcwd(), 'examples', 'inference', 'triton-inference-server', \n",
    "                            'python_backend', 'xlm-roberta-base-neuron')\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "\n",
    "# initialize key variables\n",
    "release_name = 'triton-server-xlm-roberta-base-neuronx'\n",
    "namespace = 'kubeflow-user-example-com'\n",
    "hf_model_id = 'FacebookAI/xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push Docker Container\n",
    "\n",
    "Build and push Docker container image to your current AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Access the region_name attribute to get the current region\n",
    "current_region = session.region_name\n",
    "\n",
    "cmd = ['./containers/tritonserver-neuronx/build_tools/build_and_push.sh', current_region]\n",
    "\n",
    "# Start the subprocess with streaming output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')  # end='' prevents double newlines\n",
    "    sys.stdout.flush()   # Force immediate output\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code != 0:\n",
    "    print(f\"\\nProcess exited with return code: {return_code}\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Hugging Face FacebookAI XLM Roberta Base Model Weights\n",
    "\n",
    "Below we download the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"{hf_model_id}\"}}]',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for model download to complete\n",
    "wait_for_helm_release_pods(release_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the model download job\n",
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Launch Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/serving/triton-inference-server',\n",
    "    '-f', f'{notebook_dir}/triton_server.yaml',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wait_for_triton_server(release_name, namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "services = find_matching_helm_services(release_name, namespace)\n",
    "for service in services:\n",
    "    print(f\"Service {service.metadata.name} is available.\")\n",
    "    print(f\"Service type: {service.spec.type}\")\n",
    "    print(f\"Service ports: {service.spec.ports} \")\n",
    "    print(f\"Run  'kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}' in a separate terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Deployed Model\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `kubectl port-forward svc/YOUR_SERVICE_NAME 8000:8000 -n YOUR_NAMESPACE` in a separate terminal\n",
    "- Install required packages: `pip install requests numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for testing\n",
    "! pip install requests numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration for testing\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "MODEL_NAME = \"xml-roberta-base\"  # Update this based on your model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model is Ready\n",
    "\n",
    "Below we check Triton Inference server is healthy, and the model is successfully deployed within the server, and is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_server_health(base_url: str = BASE_URL) -> bool:\n",
    "    \"\"\"Check if the Triton server is healthy and responsive\"\"\"\n",
    "    try:\n",
    "        health_url = f\"{base_url}/v2/health/ready\"\n",
    "        response = requests.get(health_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úì Triton server is healthy and ready\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚úó Triton server health check failed: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚úó Cannot connect to Triton server: {e}\")\n",
    "        print(\"\\nPlease ensure kubectl port-forward is running:\")\n",
    "        print(f\"kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}\")\n",
    "        return False\n",
    "\n",
    "def check_model_ready(base_url: str = BASE_URL, model_name: str = MODEL_NAME) -> List[str]:\n",
    "    \"\"\"Check model is ready\"\"\"\n",
    "    try:\n",
    "        model_url = f\"{base_url}/v2/models/{model_name}/ready\"\n",
    "        response = requests.get(model_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"Available model: {model_name}\")\n",
    "            return [model_name]\n",
    "        else:\n",
    "            print(f\"Failed to list models: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Cannot list models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Check server health and  model is ready\n",
    "server_healthy = check_server_health()\n",
    "if server_healthy:\n",
    "    available_models = check_model_ready()\n",
    "else:\n",
    "    available_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tests for Masked Language Model\n",
    "\n",
    "Below we define the tests for Masked Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_masked_lm(text: str, model_name: str = MODEL_NAME, base_url: str = BASE_URL) -> Dict[str, Any]:\n",
    "    \"\"\"Test a single text input with the masked language model\"\"\"\n",
    "    \n",
    "    # Triton inference endpoint\n",
    "    url = f\"{base_url}/v2/models/{model_name}/infer\"\n",
    "    \n",
    "    # Prepare the request payload for Triton\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"text_input\",\n",
    "                \"shape\": [1, 1],  # batch_size=1, sequence_length=1 (string input)\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [text]\n",
    "            }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"name\": \"logits\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send request to Triton server\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        \n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"status_code\": response.status_code,\n",
    "            \"success\": response.status_code == 200\n",
    "        }\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract logits from response\n",
    "            if \"outputs\" in response_data and len(response_data[\"outputs\"]) > 0:\n",
    "                logits_output = response_data[\"outputs\"][0]\n",
    "                logits_data = logits_output[\"data\"]\n",
    "                logits_shape = logits_output[\"shape\"]\n",
    "                \n",
    "                # Convert to numpy array for analysis\n",
    "                logits_array = np.array(logits_data).reshape(logits_shape)\n",
    "                \n",
    "                result.update({\n",
    "                    \"logits_shape\": logits_shape,\n",
    "                    \"logits_datatype\": logits_output.get(\"datatype\", \"Unknown\"),\n",
    "                    \"logits_array\": logits_array,\n",
    "                    \"logits_stats\": {\n",
    "                        \"min\": float(logits_array.min()),\n",
    "                        \"max\": float(logits_array.max()),\n",
    "                        \"mean\": float(logits_array.mean()),\n",
    "                        \"std\": float(logits_array.std())\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                # Analyze top predictions for each token\n",
    "                if len(logits_shape) >= 2:\n",
    "                    seq_len, vocab_size = logits_shape[-2], logits_shape[-1]\n",
    "                    result[\"sequence_length\"] = seq_len\n",
    "                    result[\"vocabulary_size\"] = vocab_size\n",
    "                    \n",
    "                    # Get top predictions for first few token positions\n",
    "                    top_predictions = []\n",
    "                    for token_idx in range(min(5, seq_len)):  # Show first 5 tokens\n",
    "                        token_logits = logits_array[token_idx] if len(logits_shape) == 2 else logits_array[0, token_idx]\n",
    "                        top_indices = np.argsort(token_logits)[-5:][::-1]  # Top 5 indices\n",
    "                        top_scores = token_logits[top_indices]\n",
    "                        \n",
    "                        token_predictions = {\n",
    "                            \"position\": token_idx,\n",
    "                            \"top_token_ids\": top_indices.tolist(),\n",
    "                            \"top_scores\": top_scores.tolist()\n",
    "                        }\n",
    "                        top_predictions.append(token_predictions)\n",
    "                    \n",
    "                    result[\"top_predictions\"] = top_predictions\n",
    "            else:\n",
    "                result[\"error\"] = \"No outputs found in response\"\n",
    "                result[\"raw_response\"] = response_data\n",
    "        else:\n",
    "            result[\"error\"] = response.text\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        result[\"error\"] = f\"Request failed: {e}\"\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Unexpected error: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test texts with [MASK] tokens for masked language modeling\n",
    "test_texts = [\n",
    "    \"The quick brown [MASK] jumps over the lazy dog.\",\n",
    "    \"Paris is the capital of [MASK].\",\n",
    "    \"Machine learning is a subset of [MASK] intelligence.\",\n",
    "    \"The [MASK] panda is native to China.\",\n",
    "    \"Python is a popular [MASK] language.\"\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_texts)} examples with model: {MODEL_NAME}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Tests\n",
    "\n",
    "Now we run the defined tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all examples\n",
    "if server_healthy and available_models:\n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\nTest {i}/{len(test_texts)}: {text}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = test_masked_lm(text)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úì Success - Shape: {result['logits_shape']}\")\n",
    "            print(f\"  Sequence length: {result.get('sequence_length', 'Unknown')}\")\n",
    "            print(f\"  Vocabulary size: {result.get('vocabulary_size', 'Unknown')}\")\n",
    "            print(f\"  Logits stats: min={result['logits_stats']['min']:.3f}, max={result['logits_stats']['max']:.3f}, mean={result['logits_stats']['mean']:.3f}\")\n",
    "            \n",
    "            # Show top predictions for first token (often the [MASK] token)\n",
    "            if 'top_predictions' in result and result['top_predictions']:\n",
    "                first_token_pred = result['top_predictions'][0]\n",
    "                print(f\"  Top predictions for token position {first_token_pred['position']}:\")\n",
    "                for j, (token_id, score) in enumerate(zip(first_token_pred['top_token_ids'][:3], first_token_pred['top_scores'][:3])):\n",
    "                    print(f\"    {j+1}. Token ID {token_id}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"‚úó Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Summary\n",
    "    successful_tests = sum(1 for r in results if r['success'])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Summary: {successful_tests}/{len(results)} tests passed\")\n",
    "    \n",
    "    if successful_tests == len(results):\n",
    "        print(\"üéâ All tests passed! Your masked language model is working correctly.\")\n",
    "    elif successful_tests > 0:\n",
    "        print(\"‚ö†Ô∏è  Some tests passed, but there were failures. Check the errors above.\")\n",
    "    else:\n",
    "        print(\"‚ùå All tests failed. Please check your model deployment and configuration.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot run tests - server not healthy or no models available\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure your model is deployed via Helm\")\n",
    "    print(\"2. Check that kubectl port-forward is running:\")\n",
    "    print(f\"   kubectl port-forward svc/{release_name} 8000:8000 -n {namespace}\")\n",
    "    print(\"3. Verify the service is running:\")\n",
    "    print(f\"   kubectl get pods -n {namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stop Service\n",
    "\n",
    "To stop the Triton server service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
