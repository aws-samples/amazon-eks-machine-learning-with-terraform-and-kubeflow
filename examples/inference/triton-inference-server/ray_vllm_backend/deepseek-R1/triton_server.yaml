image: 
  name:
  pull_policy: Always
lws:
  size: 4
resources:
  node_type: g6e.48xlarge
  requests:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  limits:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
ebs:
  storage: 1000Gi
  mount_path: /tmp
inline_script:
- |+
  cat > /tmp/config.pbtxt <<EOF
  backend: "vllm"

  instance_group [
    {
      count: 1
      kind: KIND_MODEL
    }
  ]
  
  EOF
- |+
  cat > /tmp/model.json <<EOF
  {
    "model": "$MODEL_PATH",
    "tokenizer": "$MODEL_PATH",
    "tokenizer_mode": "auto",
    "disable_log_requests": "true",
    "gpu_memory_utilization": 0.95,
    "dtype": "auto",
    "enforce_eager": true,
    "tensor_parallel_size": 8,
    "pipeline_parallel_size": 4,
    "max_num_seqs": 2,
    "max_model_len": 81920
  }

  EOF

pre_script: 
  - mkdir -p $LOG_ROOT
  - TP_SIZE=8
  - PP_SIZE=4
  - OUTPUT_LOG="$LOG_ROOT/triton_server.log"
  - rm -rf $MODEL_REPO
  - mkdir -p $MODEL_REPO
  - VERSION=1
  - MODEL_NAME=deepseek-r1
  - mkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION
  - cp /tmp/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json
  - cp /tmp/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt
server:
  ports:
    - name: 'http'
      value: '8000'
    - name: 'grpc'
      value: '8001'
    - name: 'metric'
      value: '8002'
    - name: 'head'
      value: '6379'
  readiness_probe:
    period_secs: 10
    failure_threshold: 3
  startup_probe:
    period_secs: 10
    failure_threshold: 360
  liveness_probe:
    period_secs: 10
    failure_threshold: 3
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: OUTPUT_ROOT
      value: /efs/home/{{ .Release.Name }}/trtllm
    - name: NCCL_SOCKET_IFNAME 
      value: "^lo,docker0"
    - name: NCCL_DEBUG
      value: "WARN"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name: "RDMAV_FORK_SAFE"
      value: "1"
    - name: MODEL_REPO
      value: /efs/home/{{ .Release.Name }}/model_repository
    - name: MODEL_PATH
      value: /fsx/pretrained-models/deepseek-ai/DeepSeek-R1
  command:
    - python3
  args:
    - server.py
    - --tp=${TP_SIZE}
    - --pp=${PP_SIZE}
    - --model-repo=${MODEL_REPO}
    - --head_port=6379
    - --grpc_port=8001
    - --http_port=8000
    - --metrics_port=8002
    - --log-file=$OUTPUT_LOG
    - --namespace=${NAMESPACE}
    - --group-key=${GROUP_KEY}
  autoscaling:
    minReplicas: 1
    maxReplicas: 1
    metrics:
      - type: Pods
        pods:
          metric:
            name: avg_time_queue_us
          target:
            type: AverageValue
            averageValue: 50
