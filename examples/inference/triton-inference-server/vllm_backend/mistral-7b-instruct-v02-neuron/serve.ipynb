{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Mistral 7B Instruct v0.2 using Triton Inference Server with vLLM on AWS Neuron\n",
    "\n",
    "This notebook shows how to serve [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model using [Triton Inference Server](https://github.com/triton-inference-server) with [vLLM backend](https://github.com/triton-inference-server/vllm_backend/tree/main) on [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Get the src directory\n",
    "src_dir = os.path.join(os.getcwd(), \"src\")\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from k8s.utils import (\n",
    "    wait_for_helm_release_pods,\n",
    "    wait_for_triton_server,\n",
    "    find_matching_helm_services\n",
    ")\n",
    "\n",
    "# Get notebook directory\n",
    "notebook_dir = os.path.join(os.getcwd(), 'examples', 'inference', 'triton-inference-server', \n",
    "                            'vllm_backend', 'mistral-7b-instruct-v02-neuron')\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "\n",
    "# initialize key variables\n",
    "release_name = 'triton-server-mistral-7b-instruct-v02-vllm-nx'\n",
    "namespace = 'kubeflow-user-example-com'\n",
    "hf_model_id = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Hugging Face Mistral 7B Instruct v0.2 Model Weights\n",
    "\n",
    "**Note:** Set your Hugging Face token below before running cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual Hugging Face token\n",
    "HF_TOKEN = None\n",
    "assert HF_TOKEN, \"Please set HF_TOKEN\"\n",
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"{hf_model_id}\"}},{{\"name\":\"HF_TOKEN\",\"value\":\"{HF_TOKEN}\"}}]',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for model download to complete\n",
    "wait_for_helm_release_pods(release_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the model download job\n",
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Launch Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/serving/triton-inference-server',\n",
    "    '-f', f'{notebook_dir}/triton_server.yaml',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_triton_server(release_name, namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = find_matching_helm_services(release_name, namespace)\n",
    "for service in services:\n",
    "    print(f\"Service {service.metadata.name} is available.\")\n",
    "    print(f\"Service type: {service.spec.type}\")\n",
    "    print(f\"Service ports: {service.spec.ports} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stop Service\n",
    "\n",
    "Run this cell when you want to stop the Triton Inference Server service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}