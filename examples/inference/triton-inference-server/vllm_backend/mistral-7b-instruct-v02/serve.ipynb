{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Mistral 7B Instruct v0.2 using Triton Inference Server with vLLM\n",
    "\n",
    "This notebook shows how to serve [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model using [Triton Inference Server](https://github.com/triton-inference-server) with [vLLM backend](https://github.com/triton-inference-server/vllm_backend/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from kubernetes import client, config\n",
    "\n",
    "# Load Kubernetes configuration\n",
    "config.load_kube_config()\n",
    "v1 = client.CoreV1Api()\n",
    "\n",
    "def find_matching_helm_pods(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find pods managed by a specific Helm release\"\"\"\n",
    "    helm_pods = v1.list_namespaced_pod(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_pods = []\n",
    "    for pod in helm_pods.items:\n",
    "        if (pod.metadata.annotations and\n",
    "            pod.metadata.annotations.get('app.kubernetes.io/managed-by') == 'Helm' and \n",
    "            pod.metadata.annotations.get('app.kubernetes.io/instance') == release_name):\n",
    "            matching_pods.append(pod)\n",
    "\n",
    "    return matching_pods\n",
    "\n",
    "def wait_for_helm_release_pods(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for all pods in a helm release to complete successfully\"\"\"\n",
    "    print(f\"Waiting for pods in release '{release_name}' to complete...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            matching_pods = find_matching_helm_pods(release_name, namespace)\n",
    "            \n",
    "            if not matching_pods:\n",
    "                print(f\"No pods found in Hem release: {release_name} waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            all_completed = True\n",
    "            for pod in matching_pods:\n",
    "                status = pod.status.phase\n",
    "                print(f\"Pod {pod.metadata.name}: {status}\")\n",
    "                \n",
    "                if status in ['Pending', 'Running']:\n",
    "                    all_completed = False\n",
    "                elif status == 'Failed':\n",
    "                    print(f\"Pod {pod.metadata.name} failed!\")\n",
    "                    return False\n",
    "            \n",
    "            if all_completed:\n",
    "                print(\"All pods completed successfully!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking pods: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for pods to complete\")\n",
    "    return False\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Hugging Face Mistral 7B Instruct v0.2 Model Weights\n",
    "\n",
    "**Note:** Set your Hugging Face token below before running cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual Hugging Face token\n",
    "HF_TOKEN = None\n",
    "assert HF_TOKEN, \"Please set HF_TOKEN\"\n",
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-mistral-7b-instruct-v02-vllm',\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"mistralai/Mistral-7B-Instruct-v0.2\"}},{{\"name\":\"HF_TOKEN\",\"value\":\"{HF_TOKEN}\"}}]',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for model download to complete\n",
    "wait_for_helm_release_pods('triton-server-mistral-7b-instruct-v02-vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the model download job\n",
    "cmd = ['helm', 'uninstall', 'triton-server-mistral-7b-instruct-v02-vllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Launch Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'triton-server-mistral-7b-instruct-v02-vllm',\n",
    "    'charts/machine-learning/serving/triton-inference-server',\n",
    "    '-f', 'examples/inference/triton-inference-server/vllm_backend/mistral-7b-instruct-v02/triton_server.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Triton server to be ready\n",
    "def wait_for_triton_server(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for Triton server pods to be running and ready\"\"\"\n",
    "    print(f\"Waiting for Triton server '{release_name}' to be ready...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            matching_pods = find_matching_helm_pods(release_name, namespace)\n",
    "            \n",
    "            if not matching_pods:\n",
    "                print(f\"No pods found in Hem release: {release_name} waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            all_ready = True\n",
    "            for pod in matching_pods:\n",
    "                status = pod.status.phase\n",
    "                ready = all(condition.status == 'True' for condition in pod.status.conditions if condition.type == 'Ready')\n",
    "                print(f\"Pod {pod.metadata.name}: {status}, Ready: {ready}\")\n",
    "                \n",
    "                if status != 'Running' or not ready:\n",
    "                    all_ready = False\n",
    "                elif status == 'Failed':\n",
    "                    print(f\"Pod {pod.metadata.name} failed!\")\n",
    "                    return False\n",
    "            \n",
    "            if all_ready:\n",
    "                print(\"Triton Inference Server Pod is Ready!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking pods: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for Triton Inference Server to be Ready\")\n",
    "    return False\n",
    "\n",
    "wait_for_triton_server('triton-server-mistral-7b-instruct-v02-vllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_helm_services(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find services managed by a specific Helm release\"\"\"\n",
    "    helm_services = v1.list_namespaced_service(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_services = []\n",
    "    for service in helm_services.items:\n",
    "        if (service.metadata.annotations and\n",
    "            service.metadata.annotations.get('app.kubernetes.io/managed-by') == 'Helm' and\n",
    "            service.metadata.annotations.get('app.kubernetes.io/instance') == release_name):\n",
    "            matching_services.append(service)\n",
    "\n",
    "    return matching_services\n",
    "\n",
    "services = find_matching_helm_services('triton-server-mistral-7b-instruct-v02-vllm')\n",
    "print(f\"Triton Inference Service service: {services}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stop Service\n",
    "\n",
    "Run this cell when you want to stop the Triton Inference Server service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = ['helm', 'uninstall', 'triton-server-mistral-7b-instruct-v02-vllm', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
