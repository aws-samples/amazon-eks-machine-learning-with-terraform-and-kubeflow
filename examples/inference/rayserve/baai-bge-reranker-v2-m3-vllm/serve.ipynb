{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve BAAI BGE Reranker v2-m3 Model\n",
    "\n",
    "This notebook shows how to use [Ray Serve](../../../charts/machine-learning/training/rayserve/) Helm chart to serve [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) model for text reranking and embedding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Get the src directory\n",
    "src_dir = os.path.join(os.getcwd(), \"src\")\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from k8s.utils import (\n",
    "    wait_for_helm_release_pods,\n",
    "    wait_for_rayservice_ready, \n",
    "    find_k8s_service\n",
    ")\n",
    "\n",
    "# Get notebook directory\n",
    "notebook_dir = os.path.join(os.getcwd(), 'examples', 'inference', 'rayserve', 'baai-bge-reranker-v2-m3-vllm')\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "\n",
    "# initialize key variables\n",
    "release_name = 'rayserve-bge-reranker-v2-m3'\n",
    "namespace = 'kubeflow-user-example-com'\n",
    "hf_model_id = 'BAAI/bge-reranker-v2-m3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push Docker Container\n",
    "\n",
    "**Note:** This step builds a custom Docker container for Ray Serve. The region is automatically detected from your AWS configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Access the region_name attribute to get the current region\n",
    "current_region = session.region_name\n",
    "\n",
    "cmd = ['./containers/ray-pytorch/build_tools/build_and_push.sh', current_region]\n",
    "\n",
    "# Start the subprocess with streaming output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')  # end='' prevents double newlines\n",
    "    sys.stdout.flush()   # Force immediate output\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code != 0:\n",
    "    print(f\"\\nProcess exited with return code: {return_code}\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Hugging Face BGE Reranker v2-m3 Pre-trained Model Weights\n",
    "\n",
    "**Note:** Set your Hugging Face token below before running cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual Hugging Face token\n",
    "hf_token = None\n",
    "assert hf_token, \"Please provide your Hugging Face token\"\n",
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"{hf_model_id}\"}},{{\"name\":\"HF_TOKEN\",\"value\":\"{hf_token}\"}}]',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for model download job to complete\n",
    "wait_for_helm_release_pods(release_name=release_name, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the Helm chart after completion\n",
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Launch Ray Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', release_name,\n",
    "    'charts/machine-learning/serving/rayserve/',\n",
    "    '-f', f'{notebook_dir}/rayservice.yaml',\n",
    "    '-n', namespace\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for RayService to be ready\n",
    "wait_for_rayservice_ready(release_name=release_name, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "service_name = f\"{release_name}-serve-svc\"\n",
    "service = find_k8s_service(service_name, namespace)\n",
    "if service:\n",
    "    print(f\"Service {service.metadata.name} is available.\")\n",
    "    print(f\"Service type: {service.spec.type}\")\n",
    "    print(f\"Service ports: {service.spec.ports} \")\n",
    "else:\n",
    "    print(f\"Service {service_name} not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the BGE Reranker Service\n",
    "\n",
    "Once the service is running, you can test it with sample reranking and embedding requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Get service endpoint (assuming port-forward or LoadBalancer)\n",
    "service_url = \"http://localhost:8000\"  # Adjust based on your setup\n",
    "\n",
    "# Test reranking functionality\n",
    "rerank_payload = {\n",
    "    \"query\": \"What is machine learning?\",\n",
    "    \"documents\": [\n",
    "        \"Machine learning is a subset of artificial intelligence\",\n",
    "        \"Cooking recipes for beginners\", \n",
    "        \"Deep learning uses neural networks for pattern recognition\",\n",
    "        \"Weather forecast for tomorrow\",\n",
    "        \"Supervised learning requires labeled training data\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{service_url}/v1/rerank\", json=rerank_payload, timeout=30)\n",
    "    print(\"Reranking Results:\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error testing reranking: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding functionality\n",
    "embed_payload = {\n",
    "    \"texts\": [\n",
    "        \"Hello world\", \n",
    "        \"Machine learning is fascinating\",\n",
    "        \"Natural language processing\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{service_url}/v1/embeddings\", json=embed_payload, timeout=30)\n",
    "    embeddings = response.json()\n",
    "    print(\"Embedding Results:\")\n",
    "    print(f\"Number of embeddings: {len(embeddings['embeddings'])}\")\n",
    "    print(f\"Embedding dimension: {len(embeddings['embeddings'][0]) if embeddings['embeddings'] else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stop Service\n",
    "\n",
    "When you're done with the service, run this cell to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['helm', 'uninstall', release_name, '-n', namespace]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
