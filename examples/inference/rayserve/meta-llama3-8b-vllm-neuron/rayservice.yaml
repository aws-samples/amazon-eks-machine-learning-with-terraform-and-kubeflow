ray:
  version: '2.49.0'
  dashboard:
    host: '0.0.0.0'
  restart_policy:
    head: Never
    worker: Never
  ports:
    - name: gcs
      port: 6379
    - name: client
      port: 10001
    - name: dashboard
      port: 8265
    - name: serve
      port: 8000
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 4
    node_type: 'inf2.48xlarge' 
  tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
  env:
    - name: NEURON_CC_FLAGS
      value: "--model-type transformer --enable-fast-loading-neuron-binaries"
    - name: NEURON_COMPILE_CACHE_URL
      value: "/tmp"
    - name: NEURON_COMPILED_ARTIFACTS
      value: /efs/home/{{ .Release.Name }}/config
    - name: VLLM_NEURON_FRAMEWORK
      value: "neuronx-distributed-inference"
  serve_config_v2: 
    serveConfigV2: |
      applications:
        - name: meta-llama3-8b-instruct
          import_path: openai_api:deployment
          runtime_env:
            env_vars:
              ENGINE_CONFIG: "/etc/engine-config/engine-config.json"
              PYTHONPATH: "/etc/framework-scripts"
          deployments:
          - name: VLLMDeployment
            max_ongoing_requests: 48
            autoscaling_config:
              min_replicas: 3
              max_replicas: 6
              target_ongoing_requests: 24
            ray_actor_options:
              resources: 
                "neuron_cores": 8
  service_unhealthy_threshold_secs: 900
  deployment_unhealthy_threshold_secs: 900
engine:
  config:
    served_model_name: "meta-llama3-8b-instruct"
    model: "/fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct"
    tokenizer: "/fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct"
    tensor_parallel_size: 8
    disable_log_stats: true
    disable_log_requests: true
    max_num_seqs: 4
    dtype: "auto"
    max_model_len: 8192
    gpu_memory_utilization: 0.9
    enforce_eager: false
    enable_prefix_caching: true
    preemption_mode: "swap"
    override_neuron_config: {"continuous_batching": {"max_num_seqs": 4,"optimized_paged_attention": true}}
image:
image_pull_policy: IfNotPresent
scheduler_name: neuron-scheduler
resources:
  min_replicas: 3
  max_replicas: 6
  requests:
    "aws.amazon.com/neuron": 4
    memory: 128Gi
    cpu: 16
  limits:
    "aws.amazon.com/neuron": 4
    memory: 180Gi
    cpu: 60
  node_type: 'inf2.48xlarge' 
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
pvc:
  - name: pv-fsx
    mount_path: /fsx
  - name: pv-efs
    mount_path: /efs