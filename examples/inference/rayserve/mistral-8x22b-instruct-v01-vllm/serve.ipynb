{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve Mistral 8x22B Instruct v0.1 Model\n",
    "\n",
    "This notebook shows how to serve [mistralai/Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) model using multi-GPU, multi-node deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kubernetes\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from kubernetes import client, config\n",
    "\n",
    "# Load Kubernetes configuration\n",
    "config.load_kube_config()\n",
    "v1 = client.CoreV1Api()\n",
    "custom_api = client.CustomObjectsApi()\n",
    "\n",
    "def find_matching_helm_pods(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find pods managed by a specific Helm release\"\"\"\n",
    "    helm_pods = v1.list_namespaced_pod(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_pods = []\n",
    "    for pod in helm_pods.items:\n",
    "        if (pod.metadata.annotations and\n",
    "            pod.metadata.annotations.get('app.kubernetes.io/managed-by') == 'Helm' and \n",
    "            pod.metadata.annotations.get('app.kubernetes.io/instance') == release_name):\n",
    "            matching_pods.append(pod)\n",
    "\n",
    "    return matching_pods\n",
    "\n",
    "def wait_for_helm_release_pods(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for all pods in a helm release to complete successfully\"\"\"\n",
    "    print(f\"Waiting for pods in release '{release_name}' to complete...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            matching_pods = find_matching_helm_pods(release_name, namespace)\n",
    "            \n",
    "            if not matching_pods:\n",
    "                print(f\"No pods found in Hem release: {release_name} waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            all_completed = True\n",
    "            for pod in matching_pods:\n",
    "                status = pod.status.phase\n",
    "                print(f\"Pod {pod.metadata.name}: {status}\")\n",
    "                \n",
    "                if status in ['Pending', 'Running']:\n",
    "                    all_completed = False\n",
    "                elif status == 'Failed':\n",
    "                    print(f\"Pod {pod.metadata.name} failed!\")\n",
    "                    return False\n",
    "            \n",
    "            if all_completed:\n",
    "                print(\"All pods completed successfully!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking pods: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for pods to complete\")\n",
    "    return False\n",
    "\n",
    "def get_rayservice_events(rayservice_name:str=None, \n",
    "                          namespace: str=\"kubeflow-user-example-com\") -> list:\n",
    "    try:\n",
    "        events = v1.list_namespaced_event(namespace=namespace)\n",
    "        rayservice_events = []\n",
    "        for event in events.items:\n",
    "            # Filter for RayService events\n",
    "            if (event.involved_object.kind == \"RayService\" and\n",
    "                event.source.component == \"rayservice-controller\"):\n",
    "                \n",
    "                if event.involved_object.name != rayservice_name:\n",
    "                    continue\n",
    "                \n",
    "                rayservice_events.append(event)\n",
    "        return rayservice_events\n",
    "\n",
    "    except client.ApiException as e:\n",
    "        print(f\"Error fetching events: {e}\")\n",
    "        return []\n",
    "    \n",
    "def is_running_event(event) -> bool:\n",
    "    return (\n",
    "        event.type == \"Normal\" and\n",
    "        event.reason == \"Running\" and\n",
    "        \"running and healthy\" in event.message.lower()\n",
    "    )\n",
    "\n",
    "def detect_running_events(rayservice_name: str = None, \n",
    "                          namespace: str=\"kubeflow-user-example-com\") -> list:\n",
    "    events = get_rayservice_events(rayservice_name=rayservice_name, namespace=namespace)\n",
    "    running_events = []\n",
    "    \n",
    "    for event in events:\n",
    "        if is_running_event(event):\n",
    "            running_events.append(event)\n",
    "    \n",
    "    return running_events\n",
    "\n",
    "def wait_for_rayservice_ready(release_name, namespace='kubeflow-user-example-com', timeout=1800):\n",
    "    \"\"\"Wait for RayService to be ready and healthy\"\"\"\n",
    "    print(f\"Waiting for RayService '{release_name}' to be ready...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            # Check RayService status\n",
    "            rayservices = custom_api.list_namespaced_custom_object(\n",
    "                group=\"ray.io\",\n",
    "                version=\"v1\",\n",
    "                namespace=namespace,\n",
    "                plural=\"rayservices\"\n",
    "            )\n",
    "            \n",
    "            matching_rayservice = None\n",
    "            for rs in rayservices['items']:\n",
    "                if (rs.get('metadata', {}).get('labels', {}).get('app.kubernetes.io/instance') == release_name):\n",
    "                    matching_rayservice = rs\n",
    "                    break\n",
    "            \n",
    "            if not matching_rayservice:\n",
    "                print(f\"No RayService found for release: {release_name}, waiting...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "            rayservice_name = matching_rayservice['metadata']['name']\n",
    "            status = matching_rayservice.get('status', {})\n",
    "            service_status = status.get('serviceStatus', 'Unknown')\n",
    "            \n",
    "            print(f\"RayService {rayservice_name}: {service_status}\")\n",
    "            \n",
    "            # Check if RayService is running\n",
    "            if service_status.lower() == 'running':\n",
    "                running_events = []\n",
    "                while not (running_events := detect_running_events(rayservice_name=rayservice_name)):\n",
    "                    print(\"Waiting for RayService event: Running and Healthy\")\n",
    "                    if (time.time() - start_time) > timeout:\n",
    "                        break\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                if running_events:\n",
    "                    print(f\"RayService {rayservice_name} in namespace {namespace} is Running and Healthy!\")\n",
    "                    return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking RayService: {e}\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Timeout waiting for RayService to be Running and Healthy\")\n",
    "    return False\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser('~/amazon-eks-machine-learning-with-terraform-and-kubeflow'))\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push Docker Container\n",
    "\n",
    "**Note:** This step builds a custom Docker container for Ray Serve. The region is automatically detected from your AWS configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Access the region_name attribute to get the current region\n",
    "current_region = session.region_name\n",
    "\n",
    "cmd = ['./containers/ray-pytorch/build_tools/build_and_push.sh', current_region]\n",
    "\n",
    "# Start the subprocess with streaming output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')  # end='' prevents double newlines\n",
    "    sys.stdout.flush()   # Force immediate output\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code != 0:\n",
    "    print(f\"\\nProcess exited with return code: {return_code}\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Hugging Face Mistral 8x22B Instruct v0.1 Pre-trained Model Weights\n",
    "\n",
    "**Note:** Set your Hugging Face Token below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual Hugging Face token\n",
    "hf_token = None\n",
    "assert hf_token, \"Please provide your Hugging Face token\"\n",
    "\n",
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'rayserve-mistral-8x22b-instruct-v01',\n",
    "    'charts/machine-learning/model-prep/hf-snapshot',\n",
    "    '--set-json', f'env=[{{\"name\":\"HF_MODEL_ID\",\"value\":\"mistralai/Mixtral-8x22B-Instruct-v0.1\"}},{{\"name\":\"HF_TOKEN\",\"value\":\"{hf_token}\"}}]',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the model download to complete\n",
    "wait_for_helm_release_pods('rayserve-mistral-8x22b-instruct-v01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the Helm chart after completion\n",
    "cmd = ['helm', 'uninstall', 'rayserve-mistral-8x22b-instruct-v01', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Ray Serve Engine Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'rayserve-mistral-8x22b-instruct-v01',\n",
    "    'charts/machine-learning/data-prep/data-process',\n",
    "    '-f', 'examples/inference/rayserve/mistral-8x22b-instruct-v01-vllm/engine_config.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the engine config to complete\n",
    "wait_for_helm_release_pods('rayserve-mistral-8x22b-instruct-v01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the Helm chart after completion\n",
    "cmd = ['helm', 'uninstall', 'rayserve-mistral-8x22b-instruct-v01', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Ray Serve Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'rayserve-mistral-8x22b-instruct-v01',\n",
    "    'charts/machine-learning/model-prep/rayserve-vllm-asyncllmengine',\n",
    "    '--set', 'engine_path=/fsx/rayserve/engines/vllm_asyncllmengine.zip',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the build engine to complete\n",
    "wait_for_helm_release_pods('rayserve-mistral-8x22b-instruct-v01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the Helm chart after completion\n",
    "cmd = ['helm', 'uninstall', 'rayserve-mistral-8x22b-instruct-v01', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Launch Ray Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    'helm', 'install', '--debug', 'rayserve-mistral-8x22b-instruct-v01',\n",
    "    'charts/machine-learning/serving/rayserve/',\n",
    "    '-f', 'examples/inference/rayserve/mistral-8x22b-instruct-v01-vllm/rayservice.yaml',\n",
    "    '-n', 'kubeflow-user-example-com'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for RayService to be ready\n",
    "wait_for_rayservice_ready('rayserve-mistral-8x22b-instruct-v01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_helm_services(release_name, namespace='kubeflow-user-example-com'):\n",
    "    \"\"\"Find services managed by a specific Helm release\"\"\"\n",
    "    helm_services = v1.list_namespaced_service(\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    matching_services = []\n",
    "    for service in helm_services.items:\n",
    "        if (service.metadata.labels and\n",
    "            service.metadata.labels.get('ray.io/service') == f\"rayservice-{release_name}\"):\n",
    "            matching_services.append(service)\n",
    "\n",
    "    return matching_services\n",
    "\n",
    "# Check service status\n",
    "services = find_matching_helm_services('rayserve-mistral-8x22b-instruct-v01')\n",
    "print(services)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Stop Service\n",
    "\n",
    "When you're done with the service, run this cell to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['helm', 'uninstall', 'rayserve-mistral-8x22b-instruct-v01', '-n', 'kubeflow-user-example-com']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
