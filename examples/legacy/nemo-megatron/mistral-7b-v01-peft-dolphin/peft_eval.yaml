image: nvcr.io/nvidia/nemo:24.12
resources:
  requests:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  limits:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  nnodes: 1
  nproc_per_node: 8 
  node_type: 'p4d.24xlarge'  
ebs:
  storage: 400Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
inline_script:
  - |+
    cat > /tmp/fix_peft_ckpt.py <<EOF
    import torch
    import tarfile
    import tempfile
    import os

    input_nemo = os.getenv("PATH_TO_TRAINED_MODEL")
    output_nemo = input_nemo.replace(".nemo", "_fixed.nemo")

    if os.path.exists(output_nemo):
        # Check if output is more recent than input
        output_mtime = os.path.getmtime(output_nemo)
        input_mtime = os.path.getmtime(input_nemo)
        
        if output_mtime > input_mtime:
            print(f"Fixed .nemo already exists and is more recent: {output_nemo}")
            exit(0)
        else:
            print(f"Fixed .nemo exists but is older than input, will regenerate: {output_nemo}")

    print(f"Fixing keys in: {input_nemo}")

    with tempfile.TemporaryDirectory() as tmpdir:
        with tarfile.open(input_nemo, "r") as tar:
            tar.extractall(tmpdir)
        
        for rank in range(8):
            ckpt_path = f"{tmpdir}/mp_rank_{rank:02d}/model_weights.ckpt"
            checkpoint = torch.load(ckpt_path, map_location='cpu')
            
            # Change 'model.decoder.' to 'model.module.decoder.'
            new_checkpoint = {}
            for key, value in checkpoint.items():
                if key.startswith('model.decoder.'):
                    new_key = key.replace('model.decoder.', 'model.module.decoder.', 1)
                else:
                    new_key = key
                new_checkpoint[new_key] = value
            
            if rank == 0:
                print(f"\nSample transformed keys (rank 0):")
                for key in sorted(list(new_checkpoint.keys()))[:3]:
                    print(f"  {key}")
            
            torch.save(new_checkpoint, ckpt_path)
        
        with tarfile.open(output_nemo, "w") as tar:
            for item in os.listdir(tmpdir):
                tar.add(os.path.join(tmpdir, item), arcname=item)

    print(f"\nFixed .nemo saved to: {output_nemo}")

    EOF
pre_script: 
  - export DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - echo "DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS"
  - export LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - mkdir -p $LOGS_DIR
  - export OUTPUT_LOG=$LOGS_DIR/peft_eval.log
  - MODEL="$MODEL_PATH/ckpt.nemo"
  - TEST_DS="[$DATA_ROOT/dolphin_test.jsonl]"
  - TEST_NAMES="[dolphin]"
  - SCHEME="lora"
  - CONCAT_SAMPLING_PROBS="[1.0]"
  - TP_SIZE=8
  - PP_SIZE=1
  - TOKENS_TO_GENERATE=700
  - OUTPUT_PREFIX=$LOG_ROOT/nemo_experiments/$EXP_NAME/eval_results
  - export PATH_TO_TRAINED_MODEL=$LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints/${EXP_NAME}.nemo
  - export HYDRA_FULL_ERROR=1
  - export PEFT_ARGS="
    trainer.precision=bf16
    trainer.devices=$PET_NPROC_PER_NODE
    trainer.num_nodes=$PET_NNODES 
    model.restore_from_path=${MODEL} 
    model.micro_batch_size=32
    model.global_batch_size=32
    ++model.mcore_gpt=True
    model.peft.restore_from_path=$LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints/${EXP_NAME}_fixed.nemo 
    model.peft.peft_scheme=${SCHEME}
    trainer.devices=${PET_NPROC_PER_NODE} 
    +trainer.limit_test_batches=100
    model.data.test_ds.file_names=${TEST_DS} 
    model.data.test_ds.names=${TEST_NAMES}
    model.data.test_ds.tokens_to_generate=${TOKENS_TO_GENERATE} 
    model.data.test_ds.num_workers=8
    model.data.test_ds.drop_last=True
    model.data.test_ds.shuffle=True
    model.tensor_model_parallel_size=${TP_SIZE} 
    model.megatron_amp_O2=True 
    model.pipeline_model_parallel_size=${PP_SIZE}
    inference.top_k=50 
    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} 
    model.answer_only_loss=True 
    model.data.test_ds.write_predictions_to_file=True"
  - SCRIPT_DIR=/opt/NeMo/examples/nlp/language_modeling/tuning
  - echo "PATH_TO_TRAINED_MODEL=$PATH_TO_TRAINED_MODEL"
  - python /tmp/fix_peft_ckpt.py
train:
  env:
    - name: HOME
      value: /tmp
    - name: TMPDIR
      value: /tmp
    - name: TMP
      value: /tmp
    - name: MODEL_PATH
      value: /fsx/pretrained-models/mistralai/Mistral-7B-v0.1
    - name: LOG_ROOT
      value: "/efs/home/{{ .Release.Name }}/logs"
    - name: DATA_ROOT
      value: "/fsx/home/{{ .Release.Name }}/data"
    - name: EXP_NAME
      value: "peft"
  command:
    -  "torchrun"
  args: 
    - $DISTRIBUTED_ARGS
    - $SCRIPT_DIR/megatron_gpt_generate.py
    - $PEFT_ARGS
    - '2>&1 | tee $OUTPUT_LOG' 
