image: nvcr.io/nvidia/nemo:24.12
resources:
  requests:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  limits:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  node_type: 'p4d.24xlarge' 
ebs:
  storage: 400Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
pre_script:
  - SCRIPT_DIR=/opt/NeMo/scripts/checkpoint_converters
  - cd $SCRIPT_DIR
  - mkdir -p $LOG_ROOT
  - OUTPUT_LOG=$LOG_ROOT/hf_to_nemo.log
post_script:
  - cp -r /tmp/ckpt.nemo $MODEL_PATH/
process:
  env:
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_PATH
      value: /fsx/pretrained-models/mistralai/Mistral-7B-v0.1
  command:
    - python3
  args:
    - convert_mistral_7b_hf_to_nemo.py
    - --input_name_or_path=$MODEL_PATH/ 
    - --output_path=/tmp/ckpt.nemo
    - '2>&1 | tee $OUTPUT_LOG'
