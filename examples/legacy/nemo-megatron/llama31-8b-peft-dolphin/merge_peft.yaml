image: nvcr.io/nvidia/nemo:24.12
resources:
  requests:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  limits:
    "nvidia.com/gpu": 8
    "vpc.amazonaws.com/efa": 4
  node_type: 'p4d.24xlarge'  
ebs:
  storage: 500Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
pre_script:
  - SCRIPT_DIR=/opt/NeMo/scripts/nlp_language_modeling/merge_lora_weights
  - cd $SCRIPT_DIR
  - mkdir -p $LOG_ROOT
  - OUTPUT_LOG=$LOG_ROOT/merge_peft.log
  - PATH_TO_BASE_MODEL=$MODEL_PATH/ckpt.nemo
  - echo "PATH_TO_BASE_MODEL=$PATH_TO_BASE_MODEL"
  - PATH_TO_PEFT_MODEL=$LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints/${EXP_NAME}_fixed.nemo
  - echo "PATH_TO_PEFT_MODEL=$PATH_TO_PEFT_MODEL"
  - PATH_TO_MERGED_MODEL=$LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints/merged_model.nemo
  - echo "PATH_TO_MERGED_MODEL=$PATH_TO_MERGED_MODEL"
process:
  env:
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_PATH
      value: /fsx/pretrained-models/meta-llama/Llama-3.1-8B
    - name: EXP_NAME
      value: peft
    - name: HF_TOKEN
      value: "{{ .Values.hf_token }}"
  command:
    - python
  args:
    - merge.py
    - trainer.accelerator=gpu
    - gpt_model_file=$PATH_TO_BASE_MODEL
    - lora_model_path=$PATH_TO_PEFT_MODEL
    - merged_model_path=$PATH_TO_MERGED_MODEL
    - '2>&1 | tee $OUTPUT_LOG'
